{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VulnerabilityClassifier_SimpleCNNwithDataLoader_Multiple_Batch.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JayJay95/JavaByteCodeGenerator/blob/master/VulnerabilityClassifier_SimpleCNNwithDataLoader_Multiple_Batch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wbjUdmjC9gs5",
        "colab_type": "code",
        "outputId": "a718fea0-e453-4237-9664-d3a78b8b23de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uWxCvr1Z9kBw",
        "colab_type": "code",
        "outputId": "56b10bcb-6833-4b7f-a95c-cbd6567d0c27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%cd /content/drive/My\\ Drive/After4thYear/Belfast/MSc\\ Cybersec/Research\\ Project/Colab Notebooks"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/After4thYear/Belfast/MSc Cybersec/Research Project/Colab Notebooks\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVM7weaq9nxm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import re\n",
        "import fnmatch\n",
        "import argparse\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data\n",
        "from torch.autograd import Variable\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import test_cases\n",
        "from copy import deepcopy\n",
        "import pdb\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eh_f2Dk49qMy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "clean_opseq_folder = '/content/drive/My Drive/After4thYear/Belfast/MSc Cybersec/Research Project/Colab Notebooks/Opseq/Clean_Opseq'\n",
        "vuln_opseq_folder = '/content/drive/My Drive/After4thYear/Belfast/MSc Cybersec/Research Project/Colab Notebooks/Opseq/Vuln_Opseq'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9mNGPASm9qq2",
        "colab_type": "code",
        "outputId": "6fdf13f6-dd4a-48a0-d8a6-b68c4572f45c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "parser = argparse.ArgumentParser(description='Vulnerability Classifier')\n",
        "# parser.add_argument('--max_opcode_seq_len', action='store', type=int, help='use different versions of network', default=8192)\n",
        "# parser.add_argument('--min_opcode_seq_len', action='store', type=int, help='use different versions of network', default=32)\n",
        "parser.add_argument('--lr', action='store', type=float, help='use different versions of network', default=1e-5)\n",
        "parser.add_argument('--epochs', action='store', type=int, help='use different versions of network', default=10)\n",
        "opt = parser.parse_args('')\n",
        "print(opt)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Namespace(epochs=10, lr=1e-05)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3mcKD8wi9svS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RsLneDR69xty",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def find_files(files, dirs=[], extensions=[]): # recursively find files in directories\n",
        "    new_dirs = []\n",
        "    for d in dirs:\n",
        "        try:\n",
        "            new_dirs += [ os.path.join(d, f) for f in os.listdir(d)] # check in all directories except testcasesupport \n",
        "        except OSError:\n",
        "            if os.path.splitext(d)[1] in extensions:\n",
        "                files.append(d)\n",
        "\n",
        "    if new_dirs:\n",
        "        find_files(files, new_dirs, extensions)\n",
        "    else:\n",
        "        return  \n",
        "\n",
        "def read_file(filename):\n",
        "    opcode_count = 0\n",
        "    line_list = []\n",
        "    with open(filename, mode='rt', encoding='utf8') as f:\n",
        "        content = f.readlines()        \n",
        "    for line in content:\n",
        "        opcode_seq = []     \n",
        "        for c in range(0, len(line) - 1, 2):\n",
        "            #print(line[c:(c+2)],int(line[c:(c+2)], 16))\n",
        "            opcode_seq.append(int(line[c:(c+2)], 16) + 1) # add one here so that the zero'th embedding is reserved for 'blank' i.e. no instruction whatsoever not even no-op\t\t\t\n",
        "            opcode_count += 1\n",
        "            # to save training time we only read \n",
        "            # the first opt.max_opcode_seq_len opcodes of each file\n",
        "            \n",
        "            # if opcode_count >= opt.max_opcode_seq_len:\n",
        "            #     return opcode_seq\n",
        "        line_list.append(opcode_seq)\n",
        "    return line_list\n",
        "\n",
        "def read_dataset():\n",
        "    vuln = []\n",
        "    clean = []\n",
        "    clean_opseq_files = []\n",
        "    vuln_opseq_files = []\n",
        "    # min_file_len = opt.min_opcode_seq_len #ignore opcode seq files shorter than this\n",
        "    find_files(clean_opseq_files, dirs=[clean_opseq_folder], extensions=['.clean'])\n",
        "    for clean_file_pathname in clean_opseq_files:\n",
        "        tmp = read_file(clean_file_pathname)\n",
        "        # if len(tmp) >= min_file_len:\n",
        "        clean.append(tmp)\n",
        "    \n",
        "    find_files(vuln_opseq_files, dirs=[vuln_opseq_folder], extensions=['.vuln'])\n",
        "    for vuln_file_pathname in vuln_opseq_files:\n",
        "        tmp = read_file(vuln_file_pathname)\n",
        "        # if len(tmp) >= min_file_len:\n",
        "        vuln.append(tmp)\n",
        "    \n",
        "    # flatten vuln and clean lists\n",
        "    new_vuln = []\n",
        "    for x in vuln:\n",
        "        for y in x:\n",
        "            new_vuln.append(y)\n",
        "    \n",
        "    new_clean = []\n",
        "    for x in clean:\n",
        "        for y in x:\n",
        "            new_clean.append(y)\n",
        "    return new_vuln, new_clean"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTQPHjW19ycq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_dataset(vuln, clean):\n",
        "    #split the dataset into train, val, test sets\n",
        "    #return the concatenated dataset and\n",
        "    #indicies pointing to the train,val,test samples\n",
        "\n",
        "    vuln_label = 0\n",
        "    clean_label = 1\n",
        "    dataset = deepcopy(clean) + deepcopy(vuln) # concatenate original clean and original vuln samples\n",
        "    trial_dataset = clean + vuln\n",
        "    \n",
        "    #pad with zeroes to make all sequences a standard length of 1000\n",
        "    for list in trial_dataset:\n",
        "        if len(list) < 1000:\n",
        "            list.extend([0] * (1000- len(list)))\n",
        "    \n",
        "    final_vuln_list = []\n",
        "    for vuln_list in vuln:\n",
        "        inner_vuln_list = []\n",
        "        inner_vuln_list.append(vuln_list)\n",
        "        inner_vuln_list.append(vuln_label)\n",
        "        final_vuln_list.append(inner_vuln_list)\n",
        "             \n",
        "    final_clean_list = []\n",
        "    for clean_list in clean:\n",
        "        inner_clean_list = []\n",
        "        inner_clean_list.append(clean_list)\n",
        "        inner_clean_list.append(clean_label)\n",
        "        final_clean_list.append(inner_clean_list)\n",
        "    \n",
        "    # split vuln samples randomly\n",
        "    first_vuln_split = int(0.8 * len(final_vuln_list))\n",
        "    second_vuln_split = int(0.1 * len(final_vuln_list))\n",
        "    third_vuln_split = int(len(final_vuln_list) - (first_vuln_split + second_vuln_split))\n",
        "    vuln_training_dataset, vuln_validation_dataset, vuln_testing_dataset = torch.utils.data.random_split(final_vuln_list, [first_vuln_split, second_vuln_split, third_vuln_split])\n",
        "\n",
        "    # split clean samples randomly\n",
        "    first_clean_split = int(0.8 * len(final_clean_list))\n",
        "    second_clean_split = int(0.1*len(final_clean_list))\n",
        "    third_clean_split = int(len(final_clean_list) - (first_clean_split + second_clean_split))\n",
        "    clean_training_dataset, clean_validation_dataset, clean_testing_dataset = torch.utils.data.random_split(final_clean_list, [first_clean_split, second_clean_split, third_clean_split])\n",
        "\n",
        "    # merge both vuln & clean training sets\n",
        "    training_set = clean_training_dataset + vuln_training_dataset\n",
        "\n",
        "    # get indices and labels from training set\n",
        "    train_inds=[]\n",
        "    train_labels=[]\n",
        "    training_counter = 0\n",
        "    \n",
        "    for list in training_set:\n",
        "        train_inds.append(training_counter)\n",
        "        training_counter+=1\n",
        "        train_labels.append(list[1])\n",
        "        \n",
        "    # merge both vuln & clean validation sets\n",
        "    validation_set = clean_validation_dataset + vuln_validation_dataset\n",
        "\n",
        "    # get indices and labels from validation set\n",
        "    val_inds = []\n",
        "    val_labels = []\n",
        "    training_set_length = len(training_set)\n",
        "    for list in validation_set:\n",
        "        val_inds.append(training_set_length)\n",
        "        training_set_length += 1\n",
        "        val_labels.append(list[1])\n",
        "\n",
        "    # merge both vuln & clean testing sets\n",
        "    testing_set = clean_testing_dataset + vuln_testing_dataset\n",
        "\n",
        "    # get indices and labels from testing set\n",
        "    test_inds = []\n",
        "    test_labels = []\n",
        "    validation_set_length = len(training_set) + len(validation_set)\n",
        "    for list in testing_set:\n",
        "        test_inds.append(validation_set_length)\n",
        "        validation_set_length += 1\n",
        "        test_labels.append(list[1])\n",
        "    \n",
        "    return trial_dataset, training_set, validation_set, testing_set\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yRKTIctC-Czz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate_network_performance(predictions, ground_truth):\n",
        "    # given an array with the predicted values and the correct values\n",
        "    # calculate the precision, recall and f-score\n",
        "    cm = get_confusion_matrix(predictions, ground_truth)\n",
        "    TP = cm[0][0]\n",
        "    FP = cm[0][1]\n",
        "    FN = cm[1][0]\n",
        "    TN = cm[1][1]\n",
        "\n",
        "    precision = TP/(TP + FP + 1e-6)\n",
        "    recall = TP/(TP + FN + 1e-6)\n",
        "    classification_accuracy = (TP+TN) / (TP + TN + FP + FN + 1e-6)\n",
        "    f_score = 2 * ((precision*recall)/(precision + recall))\n",
        "\n",
        "    return precision, recall, f_score, classification_accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3gCm2uNC9_My",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class VulnerabilityDetectorNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(VulnerabilityDetectorNetwork, self).__init__()\n",
        "\n",
        "        self.num_of_embeddings = 205\n",
        "        self.embedding_dimension = 8\n",
        "        self.channels_in = 1\n",
        "        self.channels_out = 64\n",
        "        self.hidden_nodes = 16\n",
        "        self.kernel_height_dimension = 8\n",
        "        self.kernel_width_dimension = 8\n",
        "        self.padding_height_dimension = 0\n",
        "        self.padding_width_dimension = 0\n",
        "        self.features_out = 2\n",
        "\n",
        "        self.emb1 = nn.Embedding(self.num_of_embeddings, self.embedding_dimension)\n",
        "        self.conv1 = nn.Conv2d(self.channels_in, self.channels_out, kernel_size=(self.kernel_height_dimension, self.kernel_width_dimension),\n",
        "            padding=(self.padding_height_dimension, self.padding_width_dimension))\n",
        "        self.lin1 = nn.Linear(self.channels_out, self.hidden_nodes)\n",
        "        self.lin2 = nn.Linear(self.hidden_nodes, self.features_out)\n",
        "        self.drop_out = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "#         print(x.shape)\n",
        "        x = self.emb1(x)\n",
        "#         print(x.shape)        \n",
        "        x = x.unsqueeze(1)\n",
        "#         x = x.unsqueeze(0)\n",
        "#         print(x.shape)\n",
        "        x = self.conv1(x)\n",
        "#         print(x.shape)\n",
        "        x = F.relu(x)\n",
        "#         print(x.shape)        \n",
        "        x = torch.max(x,2)[0]\n",
        "#         print(x.shape)\n",
        "        x = self.drop_out(x)\n",
        "#         print(x.shape)\n",
        "        x = x.squeeze(2)\n",
        "#         print(x.shape)\n",
        "        x = self.lin1(x)\n",
        "#         print(x.shape)\n",
        "        x = self.lin2(x)\n",
        "#         print(x.shape)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ko5Q7lMZ-FAh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_confusion_matrix(preds, truth):\n",
        "    flat_truth = torch.stack(truth).flatten()\n",
        "    flat_preds = torch.stack(preds).flatten()\n",
        "    K = len(np.unique(flat_truth)) # Number of classes \n",
        "    result = np.zeros((K, K))\n",
        "    for i in range(len(flat_truth)):\n",
        "        result[flat_preds[i]][flat_truth[i]] += 1\n",
        "    confusion_matrix = result\n",
        "    return confusion_matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jUveFs2W-HHW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_network(net, tvtLoader):\n",
        "    \n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    test_loss = 0\n",
        "    predictions = []\n",
        "    labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "#         for i in range(len(inds)):\n",
        "        for i, (opsqs, lbls) in enumerate(tvtLoader):\n",
        "            net.eval()\n",
        "            opsqs = torch.stack(opsqs).flatten()\n",
        "            opsqs_tensor = opsqs.type(torch.LongTensor)\n",
        "            lbls = lbls.type(torch.LongTensor)\n",
        "            output = net(opsqs_tensor.view(-1, 1000))\n",
        "            val,idx = torch.max(output.data,1) # max pool - max over the rows\n",
        "#             predictions.append(idx.item())\n",
        "            predictions.append(idx)\n",
        "            labels.append(lbls)\n",
        "\n",
        "            test_loss = criterion(output, lbls).item()\n",
        "\n",
        "        precision, recall, f_score, classification_accuracy = evaluate_network_performance(predictions, labels)\n",
        "\n",
        "    return precision, recall, f_score, classification_accuracy\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y5R2cVAu-WDu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_network(net, trainLoader, valLoader):\n",
        "    epochs_list = []\n",
        "    train_acc_list = []\n",
        "    val_acc_list = []\n",
        "    loss_list = []\n",
        "    \n",
        "    optimizer = optim.Adam(net.parameters(), lr=opt.lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    num_training_epochs = opt.epochs\n",
        "\n",
        "    for e in range(num_training_epochs):\n",
        "        running_loss = 0\n",
        "\n",
        "        for i, (opseqs, labels) in enumerate(trainLoader):\n",
        "            net.train() # set network into training mode\n",
        "            optimizer.zero_grad() #reset the optimizer before every loop\n",
        "            \n",
        "            opseqs = torch.stack(opseqs).flatten()\n",
        "            opseqs_tensor = opseqs.type(torch.LongTensor)\n",
        "            labels = labels.type(torch.LongTensor)\n",
        "            \n",
        "            net_output = net(opseqs_tensor.view(-1, 1000)) # forward pass of current training sample through the network\n",
        "#           net_output = net(opseqs_tensor.unsqueeze(0))\n",
        "            loss = criterion(net_output, labels) # how close network is to correct answer\n",
        "            running_loss += loss.item()\n",
        "            \n",
        "            loss.backward() # back propagation step for whole net\n",
        "            optimizer.step() # call optimizer to update network's parameters\n",
        "\n",
        "        epoch_loss = running_loss/len(trainLoader)\n",
        "        \n",
        "        # for graph purposes\n",
        "        epochs_list.append(e)\n",
        "        loss_list.append(epoch_loss)\n",
        "        \n",
        "        print('iteration ', e, ' loss ', epoch_loss)\n",
        "        print('          prec rec  f1   acc')\n",
        "         \n",
        "        precision, recall, f_score, classification_accuracy = test_network(net, trainLoader)\n",
        "        train_acc_list.append(classification_accuracy)\n",
        "        print('train set',\"{0:.10f}\".format(precision),\"{0:.10f}\".format(recall),\"{0:.10f}\".format(f_score),\"{0:.10f}\".format(classification_accuracy))\n",
        "        \n",
        "        precision, recall, f_score, classification_accuracy = test_network(net, valLoader)\n",
        "        val_acc_list.append(classification_accuracy)\n",
        "        print('val set  ',\"{0:.10f}\".format(precision),\"{0:.10f}\".format(recall),\"{0:.10f}\".format(f_score),\"{0:.10f}\".format(classification_accuracy))\n",
        "\n",
        "        print()\n",
        "\n",
        "    return net, epochs_list, train_acc_list, val_acc_list, loss_list\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ynVneEWU-Y9u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def createLoaders(train_set, val_set, test_set):\n",
        "    trainLoader = torch.utils.data.DataLoader(train_set, batch_size = 1, shuffle = True, drop_last = True)\n",
        "    valLoader = torch.utils.data.DataLoader(val_set, batch_size = 1, shuffle = True, drop_last = True)\n",
        "    testLoader = torch.utils.data.DataLoader(test_set, batch_size = 1, shuffle = False, drop_last = True)\n",
        "    return trainLoader, valLoader, testLoader\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tDgzhl33pG0D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def drawEpochsAccuracyGraph(x_epochs, y_train, y_val):\n",
        "    plt.plot(x_epochs, y_train, label = \"Train\")\n",
        "    plt.plot(x_epochs, y_val, label = \"Val\")\n",
        "\n",
        "    # naming the axes\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy (%)')\n",
        "    plt.legend()\n",
        "\n",
        "    # title\n",
        "    plt.title('Epochs vs Accuracy')\n",
        "    plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MoHXS22CMcY_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def drawEpochsLossGraph(x_epochs, y_loss):\n",
        "    plt.plot(x_epochs, y_loss, label = \"Loss\")\n",
        "    # naming the axes\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss (%)')\n",
        "    plt.legend()\n",
        "\n",
        "    # title\n",
        "    plt.title('Epochs vs Loss')\n",
        "    plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KoRlwT8G-qtZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def main():\n",
        "    # create_clean_opseq_files() # should only run the first time this code is ran\n",
        "    # print('Created clean opseq')\n",
        "    # create_vuln_opseq_files() # should only run the first time this code is ran\n",
        "    # print('Created vuln opseq')\n",
        "    vuln, clean = read_dataset()\n",
        "\n",
        "    dataset, train_set, val_set, test_set = split_dataset(vuln, clean)\n",
        "#     pdb.set_trace()\n",
        "    trainLoader, valLoader, testLoader = createLoaders(train_set, val_set, test_set)\n",
        "    \n",
        "    net = VulnerabilityDetectorNetwork()\n",
        "    # net.to(device)\n",
        "    print(net)\n",
        "    print('started training network')\n",
        "    before = time.time()\n",
        "    \n",
        "    net, epochs_list, train_acc_list, val_acc_list, loss_list = train_network(net, trainLoader, valLoader)\n",
        "    print('started testing network')\n",
        "    precision, recall, f_score, classification_accuracy = test_network(net, testLoader)\n",
        "    print('test set',\"{0:.10f}\".format(precision),\"{0:.10f}\".format(recall),\"{0:.10f}\".format(f_score),\"{0:.10f}\".format(classification_accuracy))\n",
        "    print()\n",
        "    eval_time = time.time() - before\n",
        "    print (\"Time to run: \", eval_time)\n",
        "    drawEpochsAccuracyGraph(epochs_list, train_acc_list, val_acc_list)\n",
        "    drawEpochsLossGraph(epochs_list, loss_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uWh492Xi-uYS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# print('Running Test Case: Evaluation Function')\n",
        "# test_case_result_eval_function = test_cases.test_evaluation_function(evaluate_network_performance)\n",
        "# print('Test Case Result: ', test_case_result_eval_function)\n",
        "# print()\n",
        "\n",
        "# print('Running Test Case: Splitting Dataset')\n",
        "# test_vuln,test_clean = read_dataset()\n",
        "# test_case_result_split_dataset = test_cases.test_split_dataset(split_dataset, test_vuln, test_clean)\n",
        "# print('Test Case Result: ', test_case_result_split_dataset)\n",
        "# print()\n",
        "\n",
        "# print('Running Test Case: Neural Network')\n",
        "# test_case_result_network = test_cases.test_network(VulnerabilityDetectorNetwork)\n",
        "# print('Test Case Result: ', test_case_result_network)\n",
        "# print()\n",
        "\n",
        "# if test_case_result_eval_function and test_case_result_split_dataset and test_case_result_network:\n",
        "main()\n",
        "# else:\n",
        "#     print('one or more test cases failed - quitting')\n",
        "#     quit()\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}