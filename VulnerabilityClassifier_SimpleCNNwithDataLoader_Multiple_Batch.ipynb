{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VulnerabilityClassifier_SimpleCNNwithDataLoader_Multiple_Batch.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JayJay95/JavaByteCodeGenerator/blob/master/VulnerabilityClassifier_SimpleCNNwithDataLoader_Multiple_Batch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wbjUdmjC9gs5",
        "colab_type": "code",
        "outputId": "c0f9a467-9fcf-4750-b560-db258b42db4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uWxCvr1Z9kBw",
        "colab_type": "code",
        "outputId": "fc9ad92d-ef90-460a-f0e4-29b8b3e9c6df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%cd /content/drive/My\\ Drive/After4thYear/Belfast/MSc\\ Cybersec/Research\\ Project/Colab Notebooks"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/After4thYear/Belfast/MSc Cybersec/Research Project/Colab Notebooks\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVM7weaq9nxm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import re\n",
        "import fnmatch\n",
        "import argparse\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data\n",
        "from torch.autograd import Variable\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import test_cases\n",
        "from copy import deepcopy\n",
        "import pdb\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eh_f2Dk49qMy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "clean_opseq_folder = '/content/drive/My Drive/After4thYear/Belfast/MSc Cybersec/Research Project/Colab Notebooks/Opseq/Clean_Opseq'\n",
        "vuln_opseq_folder = '/content/drive/My Drive/After4thYear/Belfast/MSc Cybersec/Research Project/Colab Notebooks/Opseq/Vuln_Opseq'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9mNGPASm9qq2",
        "colab_type": "code",
        "outputId": "2a78c708-db85-4e02-ea37-7e52d28c90fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "parser = argparse.ArgumentParser(description='Vulnerability Classifier')\n",
        "# parser.add_argument('--max_opcode_seq_len', action='store', type=int, help='use different versions of network', default=8192)\n",
        "# parser.add_argument('--min_opcode_seq_len', action='store', type=int, help='use different versions of network', default=32)\n",
        "parser.add_argument('--lr', action='store', type=float, help='use different versions of network', default=1e-5)\n",
        "parser.add_argument('--epochs', action='store', type=int, help='use different versions of network', default=30)\n",
        "opt = parser.parse_args('')\n",
        "print(opt)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Namespace(epochs=30, lr=1e-05)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3mcKD8wi9svS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RsLneDR69xty",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def find_files(files, dirs=[], extensions=[]): # recursively find files in directories\n",
        "    new_dirs = []\n",
        "    for d in dirs:\n",
        "        try:\n",
        "            new_dirs += [ os.path.join(d, f) for f in os.listdir(d)] # check in all directories except testcasesupport \n",
        "        except OSError:\n",
        "            if os.path.splitext(d)[1] in extensions:\n",
        "                files.append(d)\n",
        "\n",
        "    if new_dirs:\n",
        "        find_files(files, new_dirs, extensions)\n",
        "    else:\n",
        "        return  \n",
        "\n",
        "def read_file(filename):\n",
        "    opcode_count = 0\n",
        "    line_list = []\n",
        "    with open(filename, mode='rt', encoding='utf8') as f:\n",
        "        content = f.readlines()        \n",
        "    for line in content:\n",
        "        opcode_seq = []     \n",
        "        for c in range(0, len(line) - 1, 2):\n",
        "            #print(line[c:(c+2)],int(line[c:(c+2)], 16))\n",
        "            opcode_seq.append(int(line[c:(c+2)], 16) + 1) # add one here so that the zero'th embedding is reserved for 'blank' i.e. no instruction whatsoever not even no-op\t\t\t\n",
        "            opcode_count += 1\n",
        "            # to save training time we only read \n",
        "            # the first opt.max_opcode_seq_len opcodes of each file\n",
        "            \n",
        "            # if opcode_count >= opt.max_opcode_seq_len:\n",
        "            #     return opcode_seq\n",
        "        line_list.append(opcode_seq)\n",
        "    return line_list\n",
        "\n",
        "def read_dataset():\n",
        "    vuln = []\n",
        "    clean = []\n",
        "    clean_opseq_files = []\n",
        "    vuln_opseq_files = []\n",
        "    # min_file_len = opt.min_opcode_seq_len #ignore opcode seq files shorter than this\n",
        "    find_files(clean_opseq_files, dirs=[clean_opseq_folder], extensions=['.clean'])\n",
        "    for clean_file_pathname in clean_opseq_files:\n",
        "        tmp = read_file(clean_file_pathname)\n",
        "        # if len(tmp) >= min_file_len:\n",
        "        clean.append(tmp)\n",
        "    \n",
        "    find_files(vuln_opseq_files, dirs=[vuln_opseq_folder], extensions=['.vuln'])\n",
        "    for vuln_file_pathname in vuln_opseq_files:\n",
        "        tmp = read_file(vuln_file_pathname)\n",
        "        # if len(tmp) >= min_file_len:\n",
        "        vuln.append(tmp)\n",
        "    \n",
        "    # flatten vuln and clean lists\n",
        "    new_vuln = []\n",
        "    for x in vuln:\n",
        "        for y in x:\n",
        "            new_vuln.append(y)\n",
        "    \n",
        "    new_clean = []\n",
        "    for x in clean:\n",
        "        for y in x:\n",
        "            new_clean.append(y)\n",
        "    return new_vuln, new_clean"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTQPHjW19ycq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_dataset(vuln, clean):\n",
        "    #split the dataset into train, val, test sets\n",
        "    #return the concatenated dataset and\n",
        "    #indicies pointing to the train,val,test samples\n",
        "\n",
        "    vuln_label = 0\n",
        "    clean_label = 1\n",
        "    dataset = deepcopy(clean) + deepcopy(vuln) # concatenate original clean and original vuln samples\n",
        "    trial_dataset = clean + vuln\n",
        "    \n",
        "    #pad with zeroes to make all sequences a standard length of 1000\n",
        "    for list in trial_dataset:\n",
        "        if len(list) < 1000:\n",
        "            list.extend([0] * (1000- len(list)))\n",
        "    \n",
        "    final_vuln_list = []\n",
        "    for vuln_list in vuln:\n",
        "        inner_vuln_list = []\n",
        "        inner_vuln_list.append(vuln_list)\n",
        "        inner_vuln_list.append(vuln_label)\n",
        "        final_vuln_list.append(inner_vuln_list)\n",
        "             \n",
        "    final_clean_list = []\n",
        "    for clean_list in clean:\n",
        "        inner_clean_list = []\n",
        "        inner_clean_list.append(clean_list)\n",
        "        inner_clean_list.append(clean_label)\n",
        "        final_clean_list.append(inner_clean_list)\n",
        "    \n",
        "    # split vuln samples randomly\n",
        "    first_vuln_split = int(0.8 * len(final_vuln_list))\n",
        "    second_vuln_split = int(0.1 * len(final_vuln_list))\n",
        "    third_vuln_split = int(len(final_vuln_list) - (first_vuln_split + second_vuln_split))\n",
        "    vuln_training_dataset, vuln_validation_dataset, vuln_testing_dataset = torch.utils.data.random_split(final_vuln_list, [first_vuln_split, second_vuln_split, third_vuln_split])\n",
        "\n",
        "    # split clean samples randomly\n",
        "    first_clean_split = int(0.8 * len(final_clean_list))\n",
        "    second_clean_split = int(0.1*len(final_clean_list))\n",
        "    third_clean_split = int(len(final_clean_list) - (first_clean_split + second_clean_split))\n",
        "    clean_training_dataset, clean_validation_dataset, clean_testing_dataset = torch.utils.data.random_split(final_clean_list, [first_clean_split, second_clean_split, third_clean_split])\n",
        "\n",
        "    # merge both vuln & clean training sets\n",
        "    training_set = clean_training_dataset + vuln_training_dataset\n",
        "\n",
        "    # get indices and labels from training set\n",
        "    train_inds=[]\n",
        "    train_labels=[]\n",
        "    training_counter = 0\n",
        "    \n",
        "    for list in training_set:\n",
        "        train_inds.append(training_counter)\n",
        "        training_counter+=1\n",
        "        train_labels.append(list[1])\n",
        "        \n",
        "    # merge both vuln & clean validation sets\n",
        "    validation_set = clean_validation_dataset + vuln_validation_dataset\n",
        "\n",
        "    # get indices and labels from validation set\n",
        "    val_inds = []\n",
        "    val_labels = []\n",
        "    training_set_length = len(training_set)\n",
        "    for list in validation_set:\n",
        "        val_inds.append(training_set_length)\n",
        "        training_set_length += 1\n",
        "        val_labels.append(list[1])\n",
        "\n",
        "    # merge both vuln & clean testing sets\n",
        "    testing_set = clean_testing_dataset + vuln_testing_dataset\n",
        "\n",
        "    # get indices and labels from testing set\n",
        "    test_inds = []\n",
        "    test_labels = []\n",
        "    validation_set_length = len(training_set) + len(validation_set)\n",
        "    for list in testing_set:\n",
        "        test_inds.append(validation_set_length)\n",
        "        validation_set_length += 1\n",
        "        test_labels.append(list[1])\n",
        "    \n",
        "    return trial_dataset, training_set, validation_set, testing_set\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yRKTIctC-Czz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate_network_performance(predictions, ground_truth):\n",
        "    # given an array with the predicted values and the correct values\n",
        "    # calculate the precision, recall and f-score\n",
        "    cm = get_confusion_matrix(predictions, ground_truth)\n",
        "    TP = cm[0][0]\n",
        "    FP = cm[0][1]\n",
        "    FN = cm[1][0]\n",
        "    TN = cm[1][1]\n",
        "\n",
        "    precision = TP/(TP + FP + 1e-6)\n",
        "    recall = TP/(TP + FN + 1e-6)\n",
        "    classification_accuracy = (TP+TN) / (TP + TN + FP + FN + 1e-6)\n",
        "    f_score = 2 * ((precision*recall)/(precision + recall))\n",
        "\n",
        "    return precision, recall, f_score, classification_accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3gCm2uNC9_My",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class VulnerabilityDetectorNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(VulnerabilityDetectorNetwork, self).__init__()\n",
        "\n",
        "        self.num_of_embeddings = 205\n",
        "        self.embedding_dimension = 8\n",
        "        self.channels_in = 1\n",
        "        self.channels_out = 64\n",
        "        self.hidden_nodes = 16\n",
        "        self.kernel_height_dimension = 8\n",
        "        self.kernel_width_dimension = 8\n",
        "        self.padding_height_dimension = 0\n",
        "        self.padding_width_dimension = 0\n",
        "        self.features_out = 2\n",
        "\n",
        "        self.emb1 = nn.Embedding(self.num_of_embeddings, self.embedding_dimension)\n",
        "        self.conv1 = nn.Conv2d(self.channels_in, self.channels_out, kernel_size=(self.kernel_height_dimension, self.kernel_width_dimension),\n",
        "            padding=(self.padding_height_dimension, self.padding_width_dimension))\n",
        "        self.lin1 = nn.Linear(self.channels_out, self.hidden_nodes)\n",
        "        self.lin2 = nn.Linear(self.hidden_nodes, self.features_out)\n",
        "        self.drop_out = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "#         print(x.shape)\n",
        "        x = self.emb1(x)\n",
        "#         print(x.shape)        \n",
        "        x = x.unsqueeze(1)\n",
        "#         x = x.unsqueeze(0)\n",
        "#         print(x.shape)\n",
        "        x = self.conv1(x)\n",
        "#         print(x.shape)\n",
        "        x = F.relu(x)\n",
        "#         print(x.shape)        \n",
        "        x = torch.max(x,2)[0]\n",
        "#         print(x.shape)\n",
        "        x = self.drop_out(x)\n",
        "#         print(x.shape)\n",
        "        x = x.squeeze(2)\n",
        "#         print(x.shape)\n",
        "        x = self.lin1(x)\n",
        "#         print(x.shape)\n",
        "        x = self.lin2(x)\n",
        "#         print(x.shape)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ko5Q7lMZ-FAh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_confusion_matrix(preds, truth):\n",
        "    flat_truth = torch.stack(truth).flatten()\n",
        "    flat_preds = torch.stack(preds).flatten()\n",
        "    K = len(np.unique(flat_truth)) # Number of classes \n",
        "    result = np.zeros((K, K))\n",
        "    for i in range(len(flat_truth)):\n",
        "        result[flat_preds[i]][flat_truth[i]] += 1\n",
        "    confusion_matrix = result\n",
        "    return confusion_matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jUveFs2W-HHW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_network(net, tvtLoader):\n",
        "    \n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    test_loss = 0\n",
        "    predictions = []\n",
        "    labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "#         for i in range(len(inds)):\n",
        "        for i, (opsqs, lbls) in enumerate(tvtLoader):\n",
        "            net.eval()\n",
        "            opsqs = torch.stack(opsqs).flatten()\n",
        "            opsqs_tensor = opsqs.type(torch.LongTensor)\n",
        "            lbls = lbls.type(torch.LongTensor)\n",
        "            output = net(opsqs_tensor.view(-1, 1000))\n",
        "            val,idx = torch.max(output.data,1) # max pool - max over the rows\n",
        "#             predictions.append(idx.item())\n",
        "            predictions.append(idx)\n",
        "            labels.append(lbls)\n",
        "\n",
        "            test_loss = criterion(output, lbls).item()\n",
        "\n",
        "        precision, recall, f_score, classification_accuracy = evaluate_network_performance(predictions, labels)\n",
        "\n",
        "    return precision, recall, f_score, classification_accuracy\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y5R2cVAu-WDu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_network(net, trainLoader, valLoader):\n",
        "    epochs_list = []\n",
        "    train_acc_list = []\n",
        "    val_acc_list = []\n",
        "    loss_list = []\n",
        "    \n",
        "    optimizer = optim.Adam(net.parameters(), lr=opt.lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    num_training_epochs = opt.epochs\n",
        "\n",
        "    for e in range(num_training_epochs):\n",
        "        running_loss = 0\n",
        "\n",
        "        for i, (opseqs, labels) in enumerate(trainLoader):\n",
        "            net.train() # set network into training mode\n",
        "            optimizer.zero_grad() #reset the optimizer before every loop\n",
        "            \n",
        "            opseqs = torch.stack(opseqs).flatten()\n",
        "            opseqs_tensor = opseqs.type(torch.LongTensor)\n",
        "            labels = labels.type(torch.LongTensor)\n",
        "            \n",
        "            net_output = net(opseqs_tensor.view(-1, 1000)) # forward pass of current training sample through the network\n",
        "#           net_output = net(opseqs_tensor.unsqueeze(0))\n",
        "            loss = criterion(net_output, labels) # how close network is to correct answer\n",
        "            running_loss += loss.item()\n",
        "            \n",
        "            loss.backward() # back propagation step for whole net\n",
        "            optimizer.step() # call optimizer to update network's parameters\n",
        "\n",
        "        epoch_loss = running_loss/len(trainLoader)\n",
        "        \n",
        "        # for graph purposes\n",
        "        epochs_list.append(e)\n",
        "        loss_list.append(epoch_loss)\n",
        "        \n",
        "        print('iteration ', e, ' loss ', epoch_loss)\n",
        "        print('          prec rec  f1   acc')\n",
        "         \n",
        "        precision, recall, f_score, classification_accuracy = test_network(net, trainLoader)\n",
        "        train_acc_list.append(classification_accuracy)\n",
        "        print('train set',\"{0:.10f}\".format(precision),\"{0:.10f}\".format(recall),\"{0:.10f}\".format(f_score),\"{0:.10f}\".format(classification_accuracy))\n",
        "        \n",
        "        precision, recall, f_score, classification_accuracy = test_network(net, valLoader)\n",
        "        val_acc_list.append(classification_accuracy)\n",
        "        print('val set  ',\"{0:.10f}\".format(precision),\"{0:.10f}\".format(recall),\"{0:.10f}\".format(f_score),\"{0:.10f}\".format(classification_accuracy))\n",
        "\n",
        "        print()\n",
        "\n",
        "    return net, epochs_list, train_acc_list, val_acc_list, loss_list\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ynVneEWU-Y9u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def createLoaders(train_set, val_set, test_set):\n",
        "    trainLoader = torch.utils.data.DataLoader(train_set, batch_size = 1, shuffle = True, drop_last = True)\n",
        "    valLoader = torch.utils.data.DataLoader(val_set, batch_size = 1, shuffle = True, drop_last = True)\n",
        "    testLoader = torch.utils.data.DataLoader(test_set, batch_size = 1, shuffle = False, drop_last = True)\n",
        "    return trainLoader, valLoader, testLoader\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tDgzhl33pG0D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def drawEpochsAccuracyGraph(x_epochs, y_train, y_val):\n",
        "def drawEpochsAccuracyGraph():\n",
        "    x_epochs = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49]\n",
        "    y_train = [0.6584145344733965, 0.6581882190147962, 0.6684478531380091, 0.6997548249110486, 0.75899918274019, 0.8151631357161607, 0.8259005469186409, 0.8388005280588572, 0.8574589803123474, 0.8703966806956637, 0.8943106808210938, 0.9007858175532686, 0.9041051109460728, 0.9055258691028413, 0.9070597849889098, 0.9087068586042786, 0.9098887282214134, 0.910882001623048, 0.9127805368590838, 0.9133086062624844, 0.9142515873399856, 0.9138869679900185, 0.9146664990140861, 0.9137109448555516, 0.9150814106881866, 0.9154208838760871, 0.9156597724157207, 0.9178223423534567, 0.9166907650604553, 0.9169170805190556, 0.918262400189624, 0.9180360847310237, 0.9182498271085906, 0.9182121078654906, 0.9183378386758241, 0.9189664927274915, 0.9186144464585577, 0.9186647387826912, 0.9184761425671909, 0.9191425158619584, 0.9192808197533253, 0.9197585968325925, 0.9194191236446921, 0.9200226315342929, 0.9200980700204929, 0.9195700006170923, 0.9205507009376935, 0.920638712504927, 0.9213553781238278, 0.9213302319617612]\n",
        "    y_val = [0.6532541997129812, 0.6539583542245289, 0.6621064278581524, 0.6932904133695513, 0.7560607583989477, 0.8106830297947205, 0.8221506889827833, 0.8348254701906422, 0.8530328939892332, 0.8642993661739966, 0.8884418065699183, 0.8960869126952935, 0.8988029372398347, 0.9011165877777771, 0.902726083804172, 0.9035308318173694, 0.9047379538371655, 0.9058444823553119, 0.907956945889955, 0.908560506899853, 0.9096670354179994, 0.9090634744081014, 0.9101700029262478, 0.9097676289196491, 0.9108741574377955, 0.9111759379427445, 0.9114777184476935, 0.9143949299955342, 0.9124836534641904, 0.9128860274707891, 0.9143949299955342, 0.9140931494905851, 0.9139925559889355, 0.9137913689856361, 0.9138919624872858, 0.9147973040021328, 0.9140931494905851, 0.9143949299955342, 0.9140931494905851, 0.9144955234971838, 0.9149984910054322, 0.9151996780087315, 0.9150990845070819, 0.9155014585136806, 0.9159038325202792, 0.9150990845070819, 0.9163062065268779, 0.9163062065268779, 0.9171109545400753, 0.9173121415433747]\n",
        "    plt.plot(x_epochs, y_train, label = \"Train\")\n",
        "    plt.plot(x_epochs, y_val, label = \"Val\")\n",
        "\n",
        "    # naming the axes\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy (%)')\n",
        "    plt.legend()\n",
        "\n",
        "    # title\n",
        "    plt.title('Epochs vs Accuracy')\n",
        "    plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MoHXS22CMcY_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def drawEpochsLossGraph(x_epochs, y_loss):\n",
        "def drawEpochsLossGraph():\n",
        "    x_epochs = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49]\n",
        "    y_loss = [0.6967013273608522, 0.6745678188316391, 0.6603339555841392, 0.644877606510419, 0.6219225471516007, 0.5953997234708892, 0.5595094738751435, 0.5245517585736873, 0.47741589102812804, 0.433652581487062, 0.38796561680190367, 0.3495874577437324, 0.32915466132159205, 0.31719302154223594, 0.3063805344259221, 0.30067361567391676, 0.29598434268004364, 0.28857981132900434, 0.28737297505505643, 0.2814373940317472, 0.27817683185155595, 0.27397321228177046, 0.2725739207216876, 0.26785978908349817, 0.2669831220428971, 0.26483183544387445, 0.2633804116419876, 0.26159850699421805, 0.25787972398704156, 0.2584714300737994, 0.2539793785842021, 0.25174581744669067, 0.2525152710982629, 0.25077101407977664, 0.2513861546047585, 0.2481973221774613, 0.24791413625402378, 0.2445925750663176, 0.24443807470106782, 0.24571025536085364, 0.24276696698133685, 0.2415436951412054, 0.2394712409064405, 0.24034656522305042, 0.2396830300348922, 0.23964420243991855, 0.23741897556261227, 0.23677466290761653, 0.23669560364327336, 0.2371069494360628]\n",
        "    plt.plot(x_epochs, y_loss, label = \"Loss\")\n",
        "    # naming the axes\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss (%)')\n",
        "    plt.legend()\n",
        "\n",
        "    # title\n",
        "    plt.title('Epochs vs Loss')\n",
        "    plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UHzRSRUx4bXO",
        "colab_type": "code",
        "outputId": "576f9c60-b5a8-461f-a1b0-ca9a73dfdb73",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        }
      },
      "source": [
        "drawEpochsAccuracyGraph()\n",
        "drawEpochsLossGraph()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n54O8yHpiAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KoRlwT8G-qtZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def main():\n",
        "    # create_clean_opseq_files() # should only run the first time this code is ran\n",
        "    # print('Created clean opseq')\n",
        "    # create_vuln_opseq_files() # should only run the first time this code is ran\n",
        "    # print('Created vuln opseq')\n",
        "    vuln, clean = read_dataset()\n",
        "\n",
        "    dataset, train_set, val_set, test_set = split_dataset(vuln, clean)\n",
        "#     pdb.set_trace()\n",
        "    trainLoader, valLoader, testLoader = createLoaders(train_set, val_set, test_set)\n",
        "    \n",
        "    net = VulnerabilityDetectorNetwork()\n",
        "    # net.to(device)\n",
        "    print(net)\n",
        "    print('started training network')\n",
        "    before = time.time()\n",
        "    \n",
        "    net, epochs_list, train_acc_list, val_acc_list, loss_list = train_network(net, trainLoader, valLoader)\n",
        "    print('started testing network')\n",
        "    precision, recall, f_score, classification_accuracy = test_network(net, testLoader)\n",
        "    print('test set',\"{0:.10f}\".format(precision),\"{0:.10f}\".format(recall),\"{0:.10f}\".format(f_score),\"{0:.10f}\".format(classification_accuracy))\n",
        "    print()\n",
        "    eval_time = time.time() - before\n",
        "    print (\"Time to run: \", eval_time)\n",
        "    drawEpochsAccuracyGraph(epochs_list, train_acc_list, val_acc_list)\n",
        "    drawEpochsLossGraph(epochs_list, loss_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uWh492Xi-uYS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# print('Running Test Case: Evaluation Function')\n",
        "# test_case_result_eval_function = test_cases.test_evaluation_function(evaluate_network_performance)\n",
        "# print('Test Case Result: ', test_case_result_eval_function)\n",
        "# print()\n",
        "\n",
        "# print('Running Test Case: Splitting Dataset')\n",
        "# test_vuln,test_clean = read_dataset()\n",
        "# test_case_result_split_dataset = test_cases.test_split_dataset(split_dataset, test_vuln, test_clean)\n",
        "# print('Test Case Result: ', test_case_result_split_dataset)\n",
        "# print()\n",
        "\n",
        "# print('Running Test Case: Neural Network')\n",
        "# test_case_result_network = test_cases.test_network(VulnerabilityDetectorNetwork)\n",
        "# print('Test Case Result: ', test_case_result_network)\n",
        "# print()\n",
        "\n",
        "# if test_case_result_eval_function and test_case_result_split_dataset and test_case_result_network:\n",
        "main()\n",
        "# else:\n",
        "#     print('one or more test cases failed - quitting')\n",
        "#     quit()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lme5bgO3qQtZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from __future__ import print_function, with_statement, division\n",
        "# import copy\n",
        "# import os\n",
        "# import torch\n",
        "# from tqdm.autonotebook import tqdm\n",
        "# from torch.optim.lr_scheduler import _LRScheduler\n",
        "# import matplotlib.pyplot as plt\n",
        "# import pdb\n",
        "\n",
        "\n",
        "# class LRFinder(object):\n",
        "#     \"\"\"Learning rate range test.\n",
        "\n",
        "#     The learning rate range test increases the learning rate in a pre-training run\n",
        "#     between two boundaries in a linear or exponential manner. It provides valuable\n",
        "#     information on how well the network can be trained over a range of learning rates\n",
        "#     and what is the optimal learning rate.\n",
        "\n",
        "#     Arguments:\n",
        "#         model (torch.nn.Module): wrapped model.\n",
        "#         optimizer (torch.optim.Optimizer): wrapped optimizer where the defined learning\n",
        "#             is assumed to be the lower boundary of the range test.\n",
        "#         criterion (torch.nn.Module): wrapped loss function.\n",
        "#         device (str or torch.device, optional): a string (\"cpu\" or \"cuda\") with an\n",
        "#             optional ordinal for the device type (e.g. \"cuda:X\", where is the ordinal).\n",
        "#             Alternatively, can be an object representing the device on which the\n",
        "#             computation will take place. Default: None, uses the same device as `model`.\n",
        "#         memory_cache (boolean): if this flag is set to True, `state_dict` of model and\n",
        "#             optimizer will be cached in memory. Otherwise, they will be saved to files\n",
        "#             under the `cache_dir`.\n",
        "#         cache_dir (string): path for storing temporary files. If no path is specified,\n",
        "#             system-wide temporary directory is used.\n",
        "#             Notice that this parameter will be ignored if `memory_cache` is True.\n",
        "\n",
        "#     Example:\n",
        "#         >>> lr_finder = LRFinder(net, optimizer, criterion, device=\"cuda\")\n",
        "#         >>> lr_finder.range_test(dataloader, end_lr=100, num_iter=100)\n",
        "\n",
        "#     Cyclical Learning Rates for Training Neural Networks: https://arxiv.org/abs/1506.01186\n",
        "#     fastai/lr_find: https://github.com/fastai/fastai\n",
        "\n",
        "#     \"\"\"\n",
        "\n",
        "#     def __init__(self, model, optimizer, criterion, device=None, memory_cache=True, cache_dir=None):\n",
        "#         self.model = model\n",
        "#         self.optimizer = optimizer\n",
        "#         self.criterion = criterion\n",
        "#         self.history = {\"lr\": [], \"loss\": []}\n",
        "#         self.best_loss = None\n",
        "#         self.memory_cache = memory_cache\n",
        "#         self.cache_dir = cache_dir\n",
        "\n",
        "#         # Save the original state of the model and optimizer so they can be restored if\n",
        "#         # needed\n",
        "#         self.model_device = next(self.model.parameters()).device\n",
        "#         self.state_cacher = StateCacher(memory_cache, cache_dir=cache_dir)\n",
        "#         self.state_cacher.store('model', self.model.state_dict())\n",
        "#         self.state_cacher.store('optimizer', self.optimizer.state_dict())\n",
        "\n",
        "#         # If device is None, use the same as the model\n",
        "#         if device:\n",
        "#             self.device = device\n",
        "#         else:\n",
        "#             self.device = self.model_device\n",
        "\n",
        "#     def reset(self):\n",
        "#         \"\"\"Restores the model and optimizer to their initial states.\"\"\"\n",
        "#         self.model.load_state_dict(self.state_cacher.retrieve('model'))\n",
        "#         self.optimizer.load_state_dict(self.state_cacher.retrieve('optimizer'))\n",
        "#         self.model.to(self.model_device)\n",
        "\n",
        "#     def range_test(\n",
        "#         self,\n",
        "#         train_loader,\n",
        "#         val_loader=None,\n",
        "#         end_lr=10,\n",
        "#         num_iter=100,\n",
        "#         step_mode=\"exp\",\n",
        "#         smooth_f=0.05,\n",
        "#         diverge_th=5,\n",
        "#     ):\n",
        "#         \"\"\"Performs the learning rate range test.\n",
        "\n",
        "#         Arguments:\n",
        "#             train_loader (torch.utils.data.DataLoader): the training set data laoder.\n",
        "#             val_loader (torch.utils.data.DataLoader, optional): if `None` the range test\n",
        "#                 will only use the training loss. When given a data loader, the model is\n",
        "#                 evaluated after each iteration on that dataset and the evaluation loss\n",
        "#                 is used. Note that in this mode the test takes significantly longer but\n",
        "#                 generally produces more precise results. Default: None.\n",
        "#             end_lr (float, optional): the maximum learning rate to test. Default: 10.\n",
        "#             num_iter (int, optional): the number of iterations over which the test\n",
        "#                 occurs. Default: 100.\n",
        "#             step_mode (str, optional): one of the available learning rate policies,\n",
        "#                 linear or exponential (\"linear\", \"exp\"). Default: \"exp\".\n",
        "#             smooth_f (float, optional): the loss smoothing factor within the [0, 1[\n",
        "#                 interval. Disabled if set to 0, otherwise the loss is smoothed using\n",
        "#                 exponential smoothing. Default: 0.05.\n",
        "#             diverge_th (int, optional): the test is stopped when the loss surpasses the\n",
        "#                 threshold:  diverge_th * best_loss. Default: 5.\n",
        "\n",
        "#         \"\"\"\n",
        "#         # Reset test results\n",
        "#         self.history = {\"lr\": [], \"loss\": []}\n",
        "#         self.best_loss = None\n",
        "\n",
        "#         # Move the model to the proper device\n",
        "#         self.model.to(self.device)\n",
        "\n",
        "#         # Initialize the proper learning rate policy\n",
        "#         if step_mode.lower() == \"exp\":\n",
        "#             lr_schedule = ExponentialLR(self.optimizer, end_lr, num_iter)\n",
        "#         elif step_mode.lower() == \"linear\":\n",
        "#             lr_schedule = LinearLR(self.optimizer, end_lr, num_iter)\n",
        "#         else:\n",
        "#             raise ValueError(\"expected one of (exp, linear), got {}\".format(step_mode))\n",
        "\n",
        "#         if smooth_f < 0 or smooth_f >= 1:\n",
        "#             raise ValueError(\"smooth_f is outside the range [0, 1[\")\n",
        "\n",
        "#         # Create an iterator to get data batch by batch\n",
        "#         iterator = iter(train_loader)\n",
        "#         for iteration in tqdm(range(num_iter)):\n",
        "#             # Get a new set of inputs and labels\n",
        "#             try:\n",
        "#                 inputs, labels = next(iterator)\t\t\t\t\n",
        "\t\t\t\t\n",
        "#             except StopIteration:\n",
        "#                 iterator = iter(train_loader)\n",
        "#                 inputs, labels = next(iterator)\n",
        "\n",
        "#             # Train on batch and retrieve loss\n",
        "#             loss = self._train_batch(inputs, labels)\n",
        "#             if val_loader:\n",
        "#                 loss = self._validate(val_loader)\n",
        "\n",
        "#             # Update the learning rate\n",
        "#             lr_schedule.step()\n",
        "#             self.history[\"lr\"].append(lr_schedule.get_lr()[0])\n",
        "\n",
        "#             # Track the best loss and smooth it if smooth_f is specified\n",
        "#             if iteration == 0:\n",
        "#                 self.best_loss = loss\n",
        "#             else:\n",
        "#                 if smooth_f > 0:\n",
        "#                     loss = smooth_f * loss + (1 - smooth_f) * self.history[\"loss\"][-1]\n",
        "#                 if loss < self.best_loss:\n",
        "#                     self.best_loss = loss\n",
        "\n",
        "#             # Check if the loss has diverged; if it has, stop the test\n",
        "#             self.history[\"loss\"].append(loss)\n",
        "#             if loss > diverge_th * self.best_loss:\n",
        "#                 print(\"Stopping early, the loss has diverged\")\n",
        "#                 break\n",
        "\n",
        "#         print(\"Learning rate search finished. See the graph with {finder_name}.plot()\")\n",
        "\n",
        "#     def _train_batch(self, inputs, labels):\n",
        "#         # Set model to training mode\n",
        "#         self.model.train()\n",
        "# # \t\tinputs_tensor = inputs.type(torch.LongTensor)\n",
        "# # \t\tinputs = torch.stack(inputs_tensor).flatten()\n",
        "# # \t\tlabels = labels.type(torch.LongTensor)\n",
        "# #         pdb.set_trace()\n",
        "        \n",
        "#         # Forward pass\n",
        "#         self.optimizer.zero_grad()\n",
        "#         inputs = torch.stack(inputs).flatten()\n",
        "#         inputs = inputs.type(torch.LongTensor)\n",
        "#         # Move data to the correct device\n",
        "#         inputs = inputs.to(self.device)\n",
        "#         labels = labels.to(self.device)\n",
        "\n",
        "#         outputs = self.model(inputs.view(-1, 1000))\n",
        "#         loss = self.criterion(outputs, labels)\n",
        "\n",
        "#         # Backward pass\n",
        "#         loss.backward()\n",
        "#         self.optimizer.step()\n",
        "\n",
        "#         return loss.item()\n",
        "\n",
        "#     def _validate(self, dataloader):\n",
        "#         # Set model to evaluation mode and disable gradient computation\n",
        "#         running_loss = 0\n",
        "#         self.model.eval()\n",
        "#         with torch.no_grad():\n",
        "#             for inputs, labels in dataloader:\n",
        "#                 inputs = torch.stack(inputs).flatten()\n",
        "#                 inputs = inputs.type(torch.LongTensor)\n",
        "#                 # Move data to the correct device\n",
        "#                 inputs = inputs.to(self.device)\n",
        "#                 labels = labels.to(self.device)\n",
        "\n",
        "#                 # Forward pass and loss computation\n",
        "#                 outputs = self.model(inputs.view(-1, 1000))\n",
        "#                 loss = self.criterion(outputs, labels)\n",
        "#                 running_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "#         return running_loss / len(dataloader.dataset)\n",
        "\n",
        "#     def plot(self, skip_start=10, skip_end=5, log_lr=True):\n",
        "#         \"\"\"Plots the learning rate range test.\n",
        "\n",
        "#         Arguments:\n",
        "#             skip_start (int, optional): number of batches to trim from the start.\n",
        "#                 Default: 10.\n",
        "#             skip_end (int, optional): number of batches to trim from the start.\n",
        "#                 Default: 5.\n",
        "#             log_lr (bool, optional): True to plot the learning rate in a logarithmic\n",
        "#                 scale; otherwise, plotted in a linear scale. Default: True.\n",
        "\n",
        "#         \"\"\"\n",
        "\n",
        "#         if skip_start < 0:\n",
        "#             raise ValueError(\"skip_start cannot be negative\")\n",
        "#         if skip_end < 0:\n",
        "#             raise ValueError(\"skip_end cannot be negative\")\n",
        "\n",
        "#         # Get the data to plot from the history dictionary. Also, handle skip_end=0\n",
        "#         # properly so the behaviour is the expected\n",
        "#         lrs = self.history[\"lr\"]\n",
        "#         losses = self.history[\"loss\"]\n",
        "#         if skip_end == 0:\n",
        "#             lrs = lrs[skip_start:]\n",
        "#             losses = losses[skip_start:]\n",
        "#         else:\n",
        "#             lrs = lrs[skip_start:-skip_end]\n",
        "#             losses = losses[skip_start:-skip_end]\n",
        "\n",
        "#         # Plot loss as a function of the learning rate\n",
        "#         plt.plot(lrs, losses)\n",
        "#         if log_lr:\n",
        "#             plt.xscale(\"log\")\n",
        "#         plt.xlabel(\"Learning rate\")\n",
        "#         plt.ylabel(\"Loss\")\n",
        "#         plt.show()\n",
        "\n",
        "\n",
        "# class LinearLR(_LRScheduler):\n",
        "#     \"\"\"Linearly increases the learning rate between two boundaries over a number of\n",
        "#     iterations.\n",
        "\n",
        "#     Arguments:\n",
        "#         optimizer (torch.optim.Optimizer): wrapped optimizer.\n",
        "#         end_lr (float, optional): the initial learning rate which is the lower\n",
        "#             boundary of the test. Default: 10.\n",
        "#         num_iter (int, optional): the number of iterations over which the test\n",
        "#             occurs. Default: 100.\n",
        "#         last_epoch (int): the index of last epoch. Default: -1.\n",
        "\n",
        "#     \"\"\"\n",
        "\n",
        "#     def __init__(self, optimizer, end_lr, num_iter, last_epoch=-1):\n",
        "#         self.end_lr = end_lr\n",
        "#         self.num_iter = num_iter\n",
        "#         super(LinearLR, self).__init__(optimizer, last_epoch)\n",
        "\n",
        "#     def get_lr(self):\n",
        "#         curr_iter = self.last_epoch + 1\n",
        "#         r = curr_iter / self.num_iter\n",
        "#         return [base_lr + r * (self.end_lr - base_lr) for base_lr in self.base_lrs]\n",
        "\n",
        "\n",
        "# class ExponentialLR(_LRScheduler):\n",
        "#     \"\"\"Exponentially increases the learning rate between two boundaries over a number of\n",
        "#     iterations.\n",
        "\n",
        "#     Arguments:\n",
        "#         optimizer (torch.optim.Optimizer): wrapped optimizer.\n",
        "#         end_lr (float, optional): the initial learning rate which is the lower\n",
        "#             boundary of the test. Default: 10.\n",
        "#         num_iter (int, optional): the number of iterations over which the test\n",
        "#             occurs. Default: 100.\n",
        "#         last_epoch (int): the index of last epoch. Default: -1.\n",
        "\n",
        "#     \"\"\"\n",
        "\n",
        "#     def __init__(self, optimizer, end_lr, num_iter, last_epoch=-1):\n",
        "#         self.end_lr = end_lr\n",
        "#         self.num_iter = num_iter\n",
        "#         super(ExponentialLR, self).__init__(optimizer, last_epoch)\n",
        "\n",
        "#     def get_lr(self):\n",
        "#         curr_iter = self.last_epoch + 1\n",
        "#         r = curr_iter / self.num_iter\n",
        "#         return [base_lr * (self.end_lr / base_lr) ** r for base_lr in self.base_lrs]\n",
        "\n",
        "\n",
        "# class StateCacher(object):\n",
        "#     def __init__(self, in_memory, cache_dir=None):\n",
        "#         self.in_memory = in_memory\n",
        "#         self.cache_dir = cache_dir\n",
        "\n",
        "#         if self.cache_dir is None:\n",
        "#             import tempfile\n",
        "#             self.cache_dir = tempfile.gettempdir()\n",
        "#         else:\n",
        "#             if not os.path.isdir(self.cache_dir):\n",
        "#                 raise ValueError('Given `cache_dir` is not a valid directory.')\n",
        "\n",
        "#         self.cached = {}\n",
        "\n",
        "#     def store(self, key, state_dict):\n",
        "#         if self.in_memory:\n",
        "#             self.cached.update({key: copy.deepcopy(state_dict)})\n",
        "#         else:\n",
        "#             fn = os.path.join(self.cache_dir, 'state_{}_{}.pt'.format(key, id(self)))\n",
        "#             self.cached.update({key: fn})\n",
        "#             torch.save(state_dict, fn)\n",
        "\n",
        "#     def retrieve(self, key):\n",
        "#         if key not in self.cached:\n",
        "#             raise KeyError('Target {} was not cached.'.format(key))\n",
        "\n",
        "#         if self.in_memory:\n",
        "#             return self.cached.get(key)\n",
        "#         else:\n",
        "#             fn = self.cached.get(key)\n",
        "#             if not os.path.exists(fn):\n",
        "#                 raise RuntimeError('Failed to load state in {}. File does not exist anymore.'.format(fn))\n",
        "#             state_dict = torch.load(fn, map_location=lambda storage, location: storage)\n",
        "#             return state_dict\n",
        "\n",
        "#     def __del__(self):\n",
        "#         \"\"\"Check whether there are unused cached files existing in `cache_dir` before\n",
        "#         this instance being destroyed.\"\"\"\n",
        "#         if self.in_memory:\n",
        "#             return\n",
        "\n",
        "#         for k in self.cached:\n",
        "#             if os.path.exists(self.cached[k]):\n",
        "#                 os.remove(self.cached[k])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PbjXGcC2b6AQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# vuln, clean = read_dataset()\n",
        "\n",
        "# dataset, train_set, val_set, test_set = split_dataset(vuln, clean)\n",
        "# trainLoader, valLoader, testLoader = createLoaders(train_set, val_set, test_set)\n",
        "# net = VulnerabilityDetectorNetwork()\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "# optimizer = optim.Adam(net.parameters(), lr=1e-7, weight_decay=1e-2)\n",
        "# lr_finder = LRFinder(net, optimizer, criterion)\n",
        "# lr_finder.range_test(trainLoader, end_lr=100, num_iter=100, step_mode=\"exp\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KLncqQBscliv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# lr_finder.plot()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ILif38bLGDL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# lr_finder.reset()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_6sxa-RFLGyf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# lr_finder.range_test(trainLoader, val_loader=valLoader, end_lr=100, num_iter=100, step_mode=\"exp\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j04ICqYCLLzN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# lr_finder.plot(skip_end=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJ7JofA2LMOU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# lr_finder.reset()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}