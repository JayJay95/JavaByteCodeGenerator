{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VulnerabilityClassifier_SimpleMLP.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JayJay95/JavaByteCodeGenerator/blob/master/VulnerabilityClassifier_SimpleMLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AwMDJ3WM02FZ",
        "colab_type": "code",
        "outputId": "5f15d298-b9f9-47ff-c26e-98a9c07f8220",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jMBeQ7_q1IUb",
        "colab_type": "code",
        "outputId": "0928035f-effe-4aff-97e2-0fc567c5c243",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%cd /content/drive/My\\ Drive/After4thYear/Belfast/MSc\\ Cybersec/Research\\ Project/Colab Notebooks"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/After4thYear/Belfast/MSc Cybersec/Research Project/Colab Notebooks\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BinCw-MFxNvJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import re\n",
        "import fnmatch\n",
        "import argparse\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data\n",
        "from torch.autograd import Variable\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import test_cases\n",
        "from copy import deepcopy\n",
        "import pdb"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NLFMybwszZgN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "clean_opseq_folder = '/content/drive/My Drive/After4thYear/Belfast/MSc Cybersec/Research Project/Colab Notebooks/Opseq/Clean_Opseq'\n",
        "vuln_opseq_folder = '/content/drive/My Drive/After4thYear/Belfast/MSc Cybersec/Research Project/Colab Notebooks/Opseq/Vuln_Opseq'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHRA__zD0AAk",
        "colab_type": "code",
        "outputId": "2be57991-b848-4737-9275-4d41c83d2fbb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "parser = argparse.ArgumentParser(description='Vulnerability Classifier')\n",
        "# parser.add_argument('--max_opcode_seq_len', action='store', type=int, help='use different versions of network', default=8192)\n",
        "# parser.add_argument('--min_opcode_seq_len', action='store', type=int, help='use different versions of network', default=32)\n",
        "parser.add_argument('--lr', action='store', type=float, help='use different versions of network', default=1e-3)\n",
        "parser.add_argument('--epochs', action='store', type=int, help='use different versions of network', default=50)\n",
        "opt = parser.parse_args('')\n",
        "print(opt)"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Namespace(epochs=50, lr=0.001)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-kQu6ifE214B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6xZXHBH249h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def find_files(files, dirs=[], extensions=[]): # recursively find files in directories\n",
        "    new_dirs = []\n",
        "    for d in dirs:\n",
        "        try:\n",
        "            new_dirs += [ os.path.join(d, f) for f in os.listdir(d)] # check in all directories except testcasesupport \n",
        "        except OSError:\n",
        "            if os.path.splitext(d)[1] in extensions:\n",
        "                files.append(d)\n",
        "\n",
        "    if new_dirs:\n",
        "        find_files(files, new_dirs, extensions)\n",
        "    else:\n",
        "        return  \n",
        "\n",
        "def read_file(filename):\n",
        "    opcode_count = 0\n",
        "    line_list = []\n",
        "    with open(filename, mode='rt', encoding='utf8') as f:\n",
        "        content = f.readlines()        \n",
        "    for line in content:\n",
        "        opcode_seq = []     \n",
        "        for c in range(0, len(line) - 1, 2):\n",
        "            #print(line[c:(c+2)],int(line[c:(c+2)], 16))\n",
        "            opcode_seq.append(int(line[c:(c+2)], 16) + 1) # add one here so that the zero'th embedding is reserved for 'blank' i.e. no instruction whatsoever not even no-op\t\t\t\n",
        "            opcode_count += 1\n",
        "            # to save training time we only read \n",
        "            # the first opt.max_opcode_seq_len opcodes of each file\n",
        "            \n",
        "            # if opcode_count >= opt.max_opcode_seq_len:\n",
        "            #     return opcode_seq\n",
        "        line_list.append(opcode_seq)\n",
        "    return line_list\n",
        "\n",
        "def read_dataset():\n",
        "    vuln = []\n",
        "    clean = []\n",
        "    clean_opseq_files = []\n",
        "    vuln_opseq_files = []\n",
        "    # min_file_len = opt.min_opcode_seq_len #ignore opcode seq files shorter than this\n",
        "    find_files(clean_opseq_files, dirs=[clean_opseq_folder], extensions=['.clean'])\n",
        "    for clean_file_pathname in clean_opseq_files:\n",
        "        tmp = read_file(clean_file_pathname)\n",
        "        # if len(tmp) >= min_file_len:\n",
        "        clean.append(tmp)\n",
        "    \n",
        "    find_files(vuln_opseq_files, dirs=[vuln_opseq_folder], extensions=['.vuln'])\n",
        "    for vuln_file_pathname in vuln_opseq_files:\n",
        "        tmp = read_file(vuln_file_pathname)\n",
        "        # if len(tmp) >= min_file_len:\n",
        "        vuln.append(tmp)\n",
        "    \n",
        "    # flatten vuln and clean lists\n",
        "    new_vuln = []\n",
        "    for x in vuln:\n",
        "        for y in x:\n",
        "            new_vuln.append(y)\n",
        "    \n",
        "    new_clean = []\n",
        "    for x in clean:\n",
        "        for y in x:\n",
        "            new_clean.append(y)\n",
        "    return new_vuln, new_clean"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8zMwi8eu3KX9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_dataset(vuln, clean):\n",
        "    #split the dataset into train, val, test sets\n",
        "    #return the concatenated dataset and\n",
        "    #indicies pointing to the train,val,test samples\n",
        "\n",
        "    vuln_label = 0\n",
        "    clean_label = 1\n",
        "    dataset = deepcopy(clean) + deepcopy(vuln) # concatenate original clean and original vuln samples\n",
        "    trial_dataset = deepcopy(clean) + deepcopy(vuln) \n",
        "    \n",
        "    final_vuln_list = []\n",
        "    for vuln_list in vuln:\n",
        "        inner_vuln_list = []\n",
        "        inner_vuln_list.append(vuln_list)\n",
        "        inner_vuln_list.append(vuln_label)\n",
        "        final_vuln_list.append(inner_vuln_list)\n",
        "             \n",
        "    final_clean_list = []\n",
        "    for clean_list in clean:\n",
        "        inner_clean_list = []\n",
        "        inner_clean_list.append(clean_list)\n",
        "        inner_clean_list.append(clean_label)\n",
        "        final_clean_list.append(inner_clean_list)\n",
        "    \n",
        "    # split vuln samples randomly\n",
        "    first_vuln_split = int(0.8 * len(final_vuln_list))\n",
        "    second_vuln_split = int(0.1 * len(final_vuln_list))\n",
        "    third_vuln_split = int(len(final_vuln_list) - (first_vuln_split + second_vuln_split))\n",
        "    vuln_training_dataset, vuln_validation_dataset, vuln_testing_dataset = torch.utils.data.random_split(final_vuln_list, [first_vuln_split, second_vuln_split, third_vuln_split])\n",
        "\n",
        "    # split clean samples randomly\n",
        "    first_clean_split = int(0.8 * len(final_clean_list))\n",
        "    second_clean_split = int(0.1*len(final_clean_list))\n",
        "    third_clean_split = int(len(final_clean_list) - (first_clean_split + second_clean_split))\n",
        "    clean_training_dataset, clean_validation_dataset, clean_testing_dataset = torch.utils.data.random_split(final_clean_list, [first_clean_split, second_clean_split, third_clean_split])\n",
        "\n",
        "    # merge both vuln & clean training sets\n",
        "    training_set = clean_training_dataset + vuln_training_dataset\n",
        "\n",
        "    # get indices and labels from training set\n",
        "    train_inds=[]\n",
        "    train_labels=[]\n",
        "    training_counter = 0\n",
        "    \n",
        "    for list in training_set:\n",
        "        train_inds.append(training_counter)\n",
        "        training_counter+=1\n",
        "        train_labels.append(list[1])\n",
        "        \n",
        "    # merge both vuln & clean validation sets\n",
        "    validation_set = clean_validation_dataset + vuln_validation_dataset\n",
        "\n",
        "    # get indices and labels from validation set\n",
        "    val_inds = []\n",
        "    val_labels = []\n",
        "    training_set_length = len(training_set)\n",
        "    for list in validation_set:\n",
        "        val_inds.append(training_set_length)\n",
        "        training_set_length += 1\n",
        "        val_labels.append(list[1])\n",
        "\n",
        "    # merge both vuln & clean testing sets\n",
        "    testing_set = clean_testing_dataset + vuln_testing_dataset\n",
        "\n",
        "    # get indices and labels from testing set\n",
        "    test_inds = []\n",
        "    test_labels = []\n",
        "    validation_set_length = len(training_set) + len(validation_set)\n",
        "    for list in testing_set:\n",
        "        test_inds.append(validation_set_length)\n",
        "        validation_set_length += 1\n",
        "        test_labels.append(list[1])\n",
        "    return trial_dataset, (train_inds, train_labels), (val_inds, val_labels), (test_inds, test_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lSGdLhhV3ReV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class VulnerabilityDetectorNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(VulnerabilityDetectorNetwork, self).__init__()\n",
        "        self.emb1 = nn.Embedding(205, 8)\n",
        "        self.lin1 = nn.Linear(8000, 1000)\n",
        "        self.lin2 = nn.Linear(1000, 64)\n",
        "        self.lin3 = nn.Linear(64, 2)\n",
        "\n",
        "    def forward(self, x):        \n",
        "        x = self.emb1(x)\n",
        "        x = x.unsqueeze(0)\n",
        "        x = x.view(1, 8000)\n",
        "        x = F.relu(self.lin1(x))\n",
        "        x = F.relu(self.lin2(x))\n",
        "        x = self.lin3(x)\n",
        "        \n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WwdzcLeH3a9a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate_network_performance(predictions, ground_truth):\n",
        "    # given an array with the predicted values and the correct values\n",
        "    # calculate the precision, recall and f-score\n",
        "    cm = get_confusion_matrix(predictions, ground_truth)\n",
        "    TP = cm[0][0]\n",
        "    FP = cm[0][1]\n",
        "    FN = cm[1][0]\n",
        "    TN = cm[1][1]\n",
        "\n",
        "    precision = TP/(TP+FP)\n",
        "    recall = TP/(TP+FN)\n",
        "    classification_accuracy = (TP+TN) / (TP + TN + FP + FN)\n",
        "    f_score = 2 * ((precision*recall)/(precision + recall))\n",
        "\n",
        "    return precision, recall, f_score, classification_accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P320RXeD3eT3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_confusion_matrix(preds, truth):\n",
        "    K = len(np.unique(truth)) # Number of classes \n",
        "    result = np.zeros((K, K))\n",
        "    for i in range(len(truth)):\n",
        "        result[preds[i]][truth[i]] += 1\n",
        "    confusion_matrix = result\n",
        "    return confusion_matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pwb8Il-83gr0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_network(net, dataset, tvt_set):\n",
        "    # given a dataset and network compute the accuracy over the data_set\n",
        "\n",
        "    inds = tvt_set[0]\n",
        "    labels = tvt_set[1]\n",
        "\n",
        "    # test = torch.utils.data.TensorDataset(inds, labels)\n",
        "    # test_loader = torch.utils.data.DataLoader(test, batch_size = batch_size, shuffle = False)\n",
        "    \n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    test_loss = 0\n",
        "    predictions = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in range(len(inds)):\n",
        "        # for i, (imgs, lbls) in enumerate(test_loader):\n",
        "        #     imgs = Variable(imgs.view(-1, 1, 100))\n",
        "            net.eval()\n",
        "\n",
        "            example = torch.LongTensor(dataset[inds[i]]).unsqueeze(0)\n",
        "            target = torch.LongTensor([labels[i]])\n",
        "            \n",
        "            example, target = example.to(device), target.to(device)\n",
        "\n",
        "            output = net(example)\n",
        "            # output = net(imgs)\n",
        "            val,idx = torch.max(output,1)\n",
        "            predictions.append(idx.item())\n",
        "\n",
        "            test_loss = criterion(output, target).item()\n",
        "            # test_loss = criterion(output, lbls).item()\n",
        "\n",
        "        precision, recall, f_score, classification_accuracy = evaluate_network_performance(predictions, labels)\n",
        "\n",
        "    return precision, recall, f_score, classification_accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5caIlRa3jiY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_network(net, dataset, train_set, val_set):\n",
        "\t\n",
        "    train_inds = train_set[0]\n",
        "    train_labels = train_set[1]\n",
        "    # pdb.set_trace()\n",
        "\n",
        "    # trainIndsTensor = torch.LongTensor(train_inds)\n",
        "    # trainLblsTensor = torch.LongTensor(train_labels)\n",
        "    # train = torch.utils.data.TensorDataset(trainIndsTensor, trainLblsTensor)\n",
        "    # train_loader = torch.utils.data.DataLoader(train, batch_size = batch_size, shuffle = False)\n",
        "    \n",
        "    optimizer = optim.Adam(net.parameters(), lr=opt.lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    num_training_epochs = opt.epochs\n",
        "    # n_iters = 10000\n",
        "    # num_epochs = n_iters/(len(train_set)/batch_size)\n",
        "    # num_epochs = int(num_epochs)\n",
        "    for list in dataset:\n",
        "        if len(list) < 1000:\n",
        "            list.extend([0] * (1000- len(list)))\n",
        "\n",
        "\n",
        "    for e in range(num_training_epochs):\n",
        "        randperm = torch.randperm(len(train_inds))\n",
        "        running_loss = 0\n",
        "        for i in range(len(train_inds)):\n",
        "        # for i, (images, labels) in enumerate(train_loader):\n",
        "            # randperm = torch.randperm(len(images))\n",
        "            # train = Variable(images.view(1, 100)).squeeze(0)\n",
        "            # labels = Variable(labels)\n",
        "            net.train()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            example = torch.LongTensor(dataset[train_inds[randperm[i]]]).unsqueeze(0)\n",
        "            target = torch.LongTensor([train_labels[randperm[i]]])\n",
        "            # pdb.set_trace()\n",
        "            # print (\"Example: \", example.shape)\n",
        "            example, target = example.to(device), target.to(device)\n",
        "\n",
        "            net_output = net(example)\n",
        "            # print(\"Net: \", net_output.shape)\n",
        "            # net_output = net(train)\n",
        "            # loss = criterion(net_output, target)\n",
        "            # print(net_output)\n",
        "            # target = target.view(1,-1)\n",
        "            loss = criterion(net_output, target)\n",
        "            # loss = criterion(net_output, torch.max(target, 1)[1])\n",
        "            # max = torch.max(net_output)\n",
        "            # print (\"MAX: \", max)\n",
        "            # pdb.set_trace()\n",
        "            # target = target[0] - 1\n",
        "            \n",
        "            # loss = criterion(max, target)\n",
        "            # loss = criterion(net_output, labels)\n",
        "            # epoch_loss += loss.item()\n",
        "            running_loss += loss.item()\n",
        "            \n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        epoch_loss = running_loss/len(train_inds)\n",
        "\n",
        "        print('iteration ', e, 'loss ', epoch_loss)\n",
        "        print('          prec rec  f1   acc')\n",
        "\n",
        "        precision, recall, f_score, classification_accuracy = test_network(net, dataset, val_set)\n",
        "        print('val set  ',\"{0:.2f}\".format(precision),\"{0:.2f}\".format(recall),\"{0:.2f}\".format(f_score),\"{0:.2f}\".format(classification_accuracy))\n",
        "\n",
        "        precision, recall, f_score, classification_accuracy = test_network(net, dataset, train_set)\n",
        "        print('train set',\"{0:.2f}\".format(precision),\"{0:.2f}\".format(recall),\"{0:.2f}\".format(f_score),\"{0:.2f}\".format(classification_accuracy))\n",
        "        print()\n",
        "\n",
        "    return net"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dj2Avrud3kZV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def main():\n",
        "    # create_clean_opseq_files() # should only run the first time this code is ran\n",
        "    # print('Created clean opseq')\n",
        "    # create_vuln_opseq_files() # should only run the first time this code is ran\n",
        "    # print('Created vuln opseq')\n",
        "    vuln, clean = read_dataset()\n",
        "\n",
        "    dataset, train_set, val_set, test_set = split_dataset(vuln, clean)\n",
        "    net = VulnerabilityDetectorNetwork()\n",
        "    net.to(device)\n",
        "    print(net)\n",
        "    print('started training network')\n",
        "    net = train_network(net, dataset, train_set, val_set)\n",
        "    \n",
        "    print('started testing network')\n",
        "    precision, recall, f_score, classification_accuracy = test_network(net, dataset, test_set)\n",
        "    print('test set',\"{0:.2f}\".format(precision),\"{0:.2f}\".format(recall),\"{0:.2f}\".format(f_score),\"{0:.2f}\".format(classification_accuracy))\n",
        "    print()\n",
        "    \n",
        "# print('Running Test Case: Evaluation Function')\n",
        "# test_case_result_eval_function = test_cases.test_evaluation_function(evaluate_network_performance)\n",
        "# print('Test Case Result: ', test_case_result_eval_function)\n",
        "# print()\n",
        "\n",
        "# print('Running Test Case: Splitting Dataset')\n",
        "# test_vuln,test_clean = read_dataset()\n",
        "# test_case_result_split_dataset = test_cases.test_split_dataset(split_dataset, test_vuln, test_clean)\n",
        "# print('Test Case Result: ', test_case_result_split_dataset)\n",
        "# print()\n",
        "\n",
        "# print('Running Test Case: Neural Network')\n",
        "# test_case_result_network = test_cases.test_network(VulnerabilityDetectorNetwork)\n",
        "# print('Test Case Result: ', test_case_result_network)\n",
        "# print()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZs7kS843smr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d1c25609-577d-4f2c-ca8b-06a380e6c814"
      },
      "source": [
        "# if test_case_result_eval_function and test_case_result_split_dataset and test_case_result_network:\n",
        "# if test_case_result_eval_function and test_case_result_split_dataset:\n",
        "main()\n",
        "# else:\n",
        "#     print('one or more test cases failed - quitting')\n",
        "#     quit()"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "VulnerabilityDetectorNetwork(\n",
            "  (emb1): Embedding(205, 8)\n",
            "  (lin1): Linear(in_features=8000, out_features=1000, bias=True)\n",
            "  (lin2): Linear(in_features=1000, out_features=64, bias=True)\n",
            "  (lin3): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n",
            "started training network\n",
            "iteration  0 loss  0.38268280195802623\n",
            "          prec rec  f1   acc\n",
            "val set   0.52 0.97 0.68 0.54\n",
            "train set 0.92 0.95 0.93 0.93\n",
            "\n",
            "iteration  1 loss  0.18262666871284916\n",
            "          prec rec  f1   acc\n",
            "val set   0.56 0.89 0.68 0.59\n",
            "train set 0.95 0.84 0.89 0.90\n",
            "\n",
            "iteration  2 loss  0.17152786765631892\n",
            "          prec rec  f1   acc\n",
            "val set   0.53 0.96 0.68 0.55\n",
            "train set 0.94 0.95 0.94 0.94\n",
            "\n",
            "iteration  3 loss  0.17396456741694458\n",
            "          prec rec  f1   acc\n",
            "val set   0.53 0.97 0.68 0.55\n",
            "train set 0.93 0.97 0.95 0.95\n",
            "\n",
            "iteration  4 loss  0.1725466287004181\n",
            "          prec rec  f1   acc\n",
            "val set   0.53 0.95 0.68 0.56\n",
            "train set 0.92 0.97 0.95 0.95\n",
            "\n",
            "iteration  5 loss  0.18032839487330135\n",
            "          prec rec  f1   acc\n",
            "val set   0.54 0.93 0.68 0.57\n",
            "train set 0.95 0.95 0.95 0.95\n",
            "\n",
            "iteration  6 loss  0.19188048422688803\n",
            "          prec rec  f1   acc\n",
            "val set   0.53 0.97 0.69 0.56\n",
            "train set 0.91 0.99 0.95 0.94\n",
            "\n",
            "iteration  7 loss  0.21725666161176696\n",
            "          prec rec  f1   acc\n",
            "val set   0.54 0.88 0.67 0.57\n",
            "train set 0.95 0.93 0.94 0.94\n",
            "\n",
            "iteration  8 loss  0.2495235843704199\n",
            "          prec rec  f1   acc\n",
            "val set   0.53 0.98 0.69 0.55\n",
            "train set 0.89 0.99 0.94 0.93\n",
            "\n",
            "iteration  9 loss  0.29183239977165826\n",
            "          prec rec  f1   acc\n",
            "val set   0.54 0.96 0.69 0.57\n",
            "train set 0.89 0.98 0.93 0.93\n",
            "\n",
            "iteration  10 loss  0.3258691630571352\n",
            "          prec rec  f1   acc\n",
            "val set   0.52 0.99 0.68 0.54\n",
            "train set 0.84 1.00 0.92 0.91\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-84-633330971967>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# else:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#     print('one or more test cases failed - quitting')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#     quit()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-83-26a52781167a>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'started training network'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'started testing network'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-82-609f7c727b15>\u001b[0m in \u001b[0;36mtrain_network\u001b[0;34m(net, dataset, train_set, val_set)\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \"\"\"\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}