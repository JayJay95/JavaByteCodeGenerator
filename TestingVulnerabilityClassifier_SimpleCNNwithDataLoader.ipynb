{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TestingVulnerabilityClassifier_SimpleCNNwithDataLoader.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JayJay95/JavaByteCodeGenerator/blob/master/TestingVulnerabilityClassifier_SimpleCNNwithDataLoader.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wbjUdmjC9gs5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uWxCvr1Z9kBw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%cd /content/drive/My\\ Drive/After4thYear/Belfast/MSc\\ Cybersec/Research\\ Project/Colab Notebooks"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVM7weaq9nxm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import re\n",
        "import fnmatch\n",
        "import argparse\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data\n",
        "from torch.autograd import Variable\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import test_cases\n",
        "from copy import deepcopy\n",
        "import pdb\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "# import sys\n",
        "# sys.path.append('/content/drive/My Drive/After4thYear/Belfast/MSc Cybersec/Research Project/Colab Notebooks/')\n",
        "# from lr_finder import LRFinder\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eh_f2Dk49qMy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "clean_opseq_folder = '/content/drive/My Drive/After4thYear/Belfast/MSc Cybersec/Research Project/Colab Notebooks/Opseq/Clean_Opseq'\n",
        "vuln_opseq_folder = '/content/drive/My Drive/After4thYear/Belfast/MSc Cybersec/Research Project/Colab Notebooks/Opseq/Vuln_Opseq'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9mNGPASm9qq2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "parser = argparse.ArgumentParser(description='Vulnerability Classifier')\n",
        "# parser.add_argument('--max_opcode_seq_len', action='store', type=int, help='use different versions of network', default=8192)\n",
        "# parser.add_argument('--min_opcode_seq_len', action='store', type=int, help='use different versions of network', default=32)\n",
        "parser.add_argument('--lr', action='store', type=float, help='use different versions of network', default=1e-5)\n",
        "parser.add_argument('--epochs', action='store', type=int, help='use different versions of network', default=1)\n",
        "opt = parser.parse_args('')\n",
        "print(opt)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3mcKD8wi9svS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RsLneDR69xty",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def find_files(files, dirs=[], extensions=[]): # recursively find files in directories\n",
        "    new_dirs = []\n",
        "    for d in dirs:\n",
        "        try:\n",
        "            new_dirs += [ os.path.join(d, f) for f in os.listdir(d)] # check in all directories except testcasesupport \n",
        "        except OSError:\n",
        "            if os.path.splitext(d)[1] in extensions:\n",
        "                files.append(d)\n",
        "\n",
        "    if new_dirs:\n",
        "        find_files(files, new_dirs, extensions)\n",
        "    else:\n",
        "        return  \n",
        "\n",
        "def read_file(filename):\n",
        "    opcode_count = 0\n",
        "    line_list = []\n",
        "    with open(filename, mode='rt', encoding='utf8') as f:\n",
        "        content = f.readlines()        \n",
        "    for line in content:\n",
        "        opcode_seq = []     \n",
        "        for c in range(0, len(line) - 1, 2):\n",
        "            #print(line[c:(c+2)],int(line[c:(c+2)], 16))\n",
        "            opcode_seq.append(int(line[c:(c+2)], 16) + 1) # add one here so that the zero'th embedding is reserved for 'blank' i.e. no instruction whatsoever not even no-op\t\t\t\n",
        "            opcode_count += 1\n",
        "            # to save training time we only read \n",
        "            # the first opt.max_opcode_seq_len opcodes of each file\n",
        "            \n",
        "            # if opcode_count >= opt.max_opcode_seq_len:\n",
        "            #     return opcode_seq\n",
        "        line_list.append(opcode_seq)\n",
        "    return line_list\n",
        "\n",
        "def read_dataset():\n",
        "    vuln = []\n",
        "    clean = []\n",
        "    clean_opseq_files = []\n",
        "    vuln_opseq_files = []\n",
        "    # min_file_len = opt.min_opcode_seq_len #ignore opcode seq files shorter than this\n",
        "    find_files(clean_opseq_files, dirs=[clean_opseq_folder], extensions=['.clean'])\n",
        "    for clean_file_pathname in clean_opseq_files:\n",
        "        tmp = read_file(clean_file_pathname)\n",
        "        # if len(tmp) >= min_file_len:\n",
        "        clean.append(tmp)\n",
        "    \n",
        "    find_files(vuln_opseq_files, dirs=[vuln_opseq_folder], extensions=['.vuln'])\n",
        "    for vuln_file_pathname in vuln_opseq_files:\n",
        "        tmp = read_file(vuln_file_pathname)\n",
        "        # if len(tmp) >= min_file_len:\n",
        "        vuln.append(tmp)\n",
        "    \n",
        "    # flatten vuln and clean lists\n",
        "    new_vuln = []\n",
        "    for x in vuln:\n",
        "        for y in x:\n",
        "            new_vuln.append(y)\n",
        "    \n",
        "    new_clean = []\n",
        "    for x in clean:\n",
        "        for y in x:\n",
        "            new_clean.append(y)\n",
        "    return new_vuln, new_clean"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTQPHjW19ycq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_dataset(vuln, clean):\n",
        "    #split the dataset into train, val, test sets\n",
        "    #return the concatenated dataset and\n",
        "    #indicies pointing to the train,val,test samples\n",
        "\n",
        "    vuln_label = 0\n",
        "    clean_label = 1\n",
        "    dataset = deepcopy(clean) + deepcopy(vuln) # concatenate original clean and original vuln samples\n",
        "    trial_dataset = clean + vuln\n",
        "    \n",
        "    #pad with zeroes to make all sequences a standard length of 1000\n",
        "    for list in trial_dataset:\n",
        "        if len(list) < 1000:\n",
        "            list.extend([0] * (1000- len(list)))\n",
        "    \n",
        "    final_vuln_list = []\n",
        "    for vuln_list in vuln:\n",
        "        inner_vuln_list = []\n",
        "        inner_vuln_list.append(vuln_list)\n",
        "        inner_vuln_list.append(vuln_label)\n",
        "        final_vuln_list.append(inner_vuln_list)\n",
        "             \n",
        "    final_clean_list = []\n",
        "    for clean_list in clean:\n",
        "        inner_clean_list = []\n",
        "        inner_clean_list.append(clean_list)\n",
        "        inner_clean_list.append(clean_label)\n",
        "        final_clean_list.append(inner_clean_list)\n",
        "    \n",
        "    # split vuln samples randomly\n",
        "    first_vuln_split = int(0.8 * len(final_vuln_list))\n",
        "    second_vuln_split = int(0.1 * len(final_vuln_list))\n",
        "    third_vuln_split = int(len(final_vuln_list) - (first_vuln_split + second_vuln_split))\n",
        "    vuln_training_dataset, vuln_validation_dataset, vuln_testing_dataset = torch.utils.data.random_split(final_vuln_list, [first_vuln_split, second_vuln_split, third_vuln_split])\n",
        "\n",
        "    # split clean samples randomly\n",
        "    first_clean_split = int(0.8 * len(final_clean_list))\n",
        "    second_clean_split = int(0.1*len(final_clean_list))\n",
        "    third_clean_split = int(len(final_clean_list) - (first_clean_split + second_clean_split))\n",
        "    clean_training_dataset, clean_validation_dataset, clean_testing_dataset = torch.utils.data.random_split(final_clean_list, [first_clean_split, second_clean_split, third_clean_split])\n",
        "\n",
        "    # merge both vuln & clean training sets\n",
        "    training_set = clean_training_dataset + vuln_training_dataset\n",
        "\n",
        "    # get indices and labels from training set\n",
        "    train_inds=[]\n",
        "    train_labels=[]\n",
        "    training_counter = 0\n",
        "    \n",
        "    for list in training_set:\n",
        "        train_inds.append(training_counter)\n",
        "        training_counter+=1\n",
        "        train_labels.append(list[1])\n",
        "        \n",
        "    # merge both vuln & clean validation sets\n",
        "    validation_set = clean_validation_dataset + vuln_validation_dataset\n",
        "\n",
        "    # get indices and labels from validation set\n",
        "    val_inds = []\n",
        "    val_labels = []\n",
        "    training_set_length = len(training_set)\n",
        "    for list in validation_set:\n",
        "        val_inds.append(training_set_length)\n",
        "        training_set_length += 1\n",
        "        val_labels.append(list[1])\n",
        "\n",
        "    # merge both vuln & clean testing sets\n",
        "    testing_set = clean_testing_dataset + vuln_testing_dataset\n",
        "\n",
        "    # get indices and labels from testing set\n",
        "    test_inds = []\n",
        "    test_labels = []\n",
        "    validation_set_length = len(training_set) + len(validation_set)\n",
        "    for list in testing_set:\n",
        "        test_inds.append(validation_set_length)\n",
        "        validation_set_length += 1\n",
        "        test_labels.append(list[1])\n",
        "#     pdb.set_trace()\n",
        "    return trial_dataset, training_set, validation_set, testing_set\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yRKTIctC-Czz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate_network_performance(predictions, ground_truth):\n",
        "    # given an array with the predicted values and the correct values\n",
        "    # calculate the precision, recall and f-score\n",
        "    cm = get_confusion_matrix(predictions, ground_truth)\n",
        "    TP = cm[0][0]\n",
        "    FP = cm[0][1]\n",
        "    FN = cm[1][0]\n",
        "    TN = cm[1][1]\n",
        "\n",
        "    precision = TP/(TP + FP + 1e-6)\n",
        "    recall = TP/(TP + FN + 1e-6)\n",
        "    classification_accuracy = (TP+TN) / (TP + TN + FP + FN + 1e-6)\n",
        "    f_score = 2 * ((precision*recall)/(precision + recall))\n",
        "\n",
        "    return precision, recall, f_score, classification_accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3gCm2uNC9_My",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class VulnerabilityDetectorNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(VulnerabilityDetectorNetwork, self).__init__()\n",
        "\n",
        "        self.num_of_embeddings = 205\n",
        "        self.embedding_dimension = 8\n",
        "        self.channels_in = 1\n",
        "        self.channels_out = 64\n",
        "        self.channels_out_two = 128\n",
        "        self.hidden_nodes = 16\n",
        "        self.kernel_height_dimension = 8\n",
        "        self.kernel_width_dimension = 8\n",
        "        self.padding_height_dimension = 0\n",
        "        self.padding_width_dimension = 0\n",
        "        self.features_out = 2\n",
        "\n",
        "        self.emb1 = nn.Embedding(self.num_of_embeddings, self.embedding_dimension)\n",
        "        self.conv1 = nn.Conv2d(self.channels_in, self.channels_out, kernel_size=(self.kernel_height_dimension, self.kernel_width_dimension),\n",
        "            padding=(self.padding_height_dimension, self.padding_width_dimension))\n",
        "#         self.conv2 = nn.Conv2d(self.channels_out, self.channels_out_two, kernel_size=(1,1),\n",
        "#             padding=(self.padding_height_dimension, self.padding_width_dimension))\n",
        "#         self.batchNorm = nn.BatchNorm2d(64)\n",
        "        self.lin1 = nn.Linear(self.channels_out, self.hidden_nodes)\n",
        "        self.lin2 = nn.Linear(self.hidden_nodes, self.features_out)\n",
        "        self.drop_out = nn.Dropout(0.5)\n",
        "#         self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "#       print(x.shape)\n",
        "        x = self.emb1(x)\n",
        "#         print(x.shape)        \n",
        "        x = x.unsqueeze(1)\n",
        "#         x = x.unsqueeze(0)\n",
        "#         print(x.shape)\n",
        "        x = self.conv1(x)\n",
        "#         print(x.shape)\n",
        "#         x = self.batchNorm(x)\n",
        "#         print (x.shape)\n",
        "        x = F.relu(x)\n",
        "#         print(x.shape)\n",
        "#         x = self.conv2(x)\n",
        "# #         print(x.shape)\n",
        "# # #         x = self.batchNorm(x)\n",
        "# # #         print (x.shape)\n",
        "#         x = F.relu(x)\n",
        "# #         print(x.shape)\n",
        "        x = torch.max(x,2)[0]\n",
        "#         print(x.shape)\n",
        "        x = self.drop_out(x)\n",
        "#         print(x.shape)\n",
        "        x = x.squeeze(2)\n",
        "#         print(x.shape)\n",
        "        x = self.lin1(x)\n",
        "#         print(x.shape)\n",
        "        x = self.lin2(x)\n",
        "#         print(x.shape)\n",
        "#         x = self.softmax(x)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ko5Q7lMZ-FAh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_confusion_matrix(preds, truth):\n",
        "    flat_truth = torch.stack(truth).flatten()\n",
        "    flat_preds = torch.stack(preds).flatten()\n",
        "    K = len(np.unique(flat_truth)) # Number of classes \n",
        "    result = np.zeros((K, K))\n",
        "    for i in range(len(flat_truth)):\n",
        "        result[flat_preds[i]][flat_truth[i]] += 1\n",
        "    confusion_matrix = result\n",
        "    print(confusion_matrix)\n",
        "    return confusion_matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jUveFs2W-HHW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def test_network(net, tvtLoader):\n",
        "    \n",
        "#     criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "#     test_loss = 0\n",
        "#     predictions = []\n",
        "#     labels = []\n",
        "\n",
        "#     with torch.no_grad():\n",
        "# #         for i in range(len(inds)):\n",
        "#         for i, (opsqs, lbls) in enumerate(tvtLoader):\n",
        "#             net.eval()\n",
        "#             opsqs = torch.stack(opsqs).flatten()\n",
        "#             opsqs_tensor = opsqs.type(torch.LongTensor)\n",
        "#             lbls = lbls.type(torch.LongTensor)\n",
        "            \n",
        "# #             output = net(opsqs_tensor.unsqueeze(0))\n",
        "#             opsqs_tensor = opsqs_tensor.view(-1, 1000)\n",
        "#             output = net(opsqs_tensor)\n",
        "# #             output = net(opsqs_tensor.view(1, -1))\n",
        "#             val,idx = torch.max(output.data,1) # max pool - max over the rows\n",
        "# #             predictions.append(idx.item())\n",
        "#             predictions.append(idx)\n",
        "#             labels.append(lbls)\n",
        "\n",
        "#             test_loss = criterion(output, lbls).item()\n",
        "\n",
        "#         precision, recall, f_score, classification_accuracy = evaluate_network_performance(predictions, labels)\n",
        "\n",
        "#     return precision, recall, f_score, classification_accuracy\n",
        "\n",
        "def test_network(net, tvtLoader):\n",
        "    \n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    test_loss = 0\n",
        "    predictions = []\n",
        "    labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "#         for i in range(len(inds)):\n",
        "        for i, (opsqs, lbls) in enumerate(tvtLoader):\n",
        "            net.eval()\n",
        "            opsqs = torch.stack(opsqs).flatten()\n",
        "            opsqs_tensor = opsqs.type(torch.LongTensor)\n",
        "            lbls = lbls.type(torch.LongTensor)\n",
        "            output = net(opsqs_tensor.view(-1, 1000))\n",
        "            val,idx = torch.max(output.data,1) # max pool - max over the rows\n",
        "#             predictions.append(idx.item())\n",
        "            predictions.append(idx)\n",
        "            labels.append(lbls)\n",
        "\n",
        "            test_loss = criterion(output, lbls).item()\n",
        "\n",
        "        precision, recall, f_score, classification_accuracy = evaluate_network_performance(predictions, labels)\n",
        "\n",
        "    return precision, recall, f_score, classification_accuracy\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y5R2cVAu-WDu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def train_network(net, trainLoader, valLoader):\n",
        "#     epochs_list = []\n",
        "#     train_acc_list = []\n",
        "#     val_acc_list = []\n",
        "    \n",
        "#     optimizer = optim.Adam(net.parameters(), lr=opt.lr)\n",
        "#     criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "#     num_training_epochs = opt.epochs\n",
        "\n",
        "\n",
        "#     for e in range(num_training_epochs):\n",
        "#         running_loss = 0\n",
        "\n",
        "#         for i, (opseqs, labels) in enumerate(trainLoader):\n",
        "#             net.train() # set network into training mode\n",
        "#             optimizer.zero_grad() #reset the optimizer before every loop\n",
        "            \n",
        "#             opseqs = torch.stack(opseqs).flatten()\n",
        "#             opseqs_tensor = opseqs.type(torch.LongTensor)\n",
        "#             labels = labels.type(torch.LongTensor)\n",
        "#             opseqs_tensor = opseqs_tensor.view(-1, 1000)\n",
        "# #             pdb.set_trace()\n",
        "#             net_output = net(opseqs_tensor)\n",
        "# #             net_output = net(opseqs_tensor.view(1, -1)) # forward pass of current training sample through the network\n",
        "# #             net_output = net(opseqs_tensor.unsqueeze(0))\n",
        "#             labels = Variable(labels)\n",
        "#             loss = criterion(net_output, labels) # how close network is to correct answer\n",
        "#             running_loss += loss.item()\n",
        "            \n",
        "#             loss.backward() # back propagation step for whole net\n",
        "#             optimizer.step() # call optimizer to update network's parameters\n",
        "\n",
        "#         epoch_loss = running_loss/len(trainLoader)\n",
        "#         epochs_list.append(e)\n",
        "\n",
        "#         print('iteration ', e, 'loss ', epoch_loss)\n",
        "#         print('          prec rec  f1   acc')\n",
        "\n",
        "#         precision, recall, f_score, classification_accuracy = test_network(net, trainLoader)\n",
        "#         val_acc_list.append(classification_accuracy)\n",
        "#         print('train set',\"{0:.2f}\".format(precision),\"{0:.2f}\".format(recall),\"{0:.2f}\".format(f_score),\"{0:.2f}\".format(classification_accuracy))\n",
        "        \n",
        "#         precision, recall, f_score, classification_accuracy = test_network(net, valLoader)\n",
        "#         train_acc_list.append(classification_accuracy)\n",
        "#         print('val set  ',\"{0:.2f}\".format(precision),\"{0:.2f}\".format(recall),\"{0:.2f}\".format(f_score),\"{0:.2f}\".format(classification_accuracy))\n",
        "\n",
        "#         print()\n",
        "\n",
        "#     return net, epochs_list, train_acc_list, val_acc_list\n",
        "\n",
        "def train_network(net, trainLoader, valLoader):\n",
        "    epochs_list = []\n",
        "    train_acc_list = []\n",
        "    val_acc_list = []\n",
        "    loss_list = []\n",
        "    \n",
        "    optimizer = optim.Adam(net.parameters(), lr=opt.lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    num_training_epochs = opt.epochs\n",
        "\n",
        "    for e in range(num_training_epochs):\n",
        "        running_loss = 0\n",
        "\n",
        "        for i, (opseqs, labels) in enumerate(trainLoader):\n",
        "            net.train() # set network into training mode\n",
        "            optimizer.zero_grad() #reset the optimizer before every loop\n",
        "            \n",
        "            opseqs = torch.stack(opseqs).flatten()\n",
        "            opseqs_tensor = opseqs.type(torch.LongTensor)\n",
        "            labels = labels.type(torch.LongTensor)\n",
        "            \n",
        "            net_output = net(opseqs_tensor.view(-1, 1000)) # forward pass of current training sample through the network\n",
        "#           net_output = net(opseqs_tensor.unsqueeze(0))\n",
        "            loss = criterion(net_output, labels) # how close network is to correct answer\n",
        "            running_loss += loss.item()\n",
        "            \n",
        "            loss.backward() # back propagation step for whole net\n",
        "            optimizer.step() # call optimizer to update network's parameters\n",
        "\n",
        "        epoch_loss = running_loss/len(trainLoader)\n",
        "        epochs_list.append(e)\n",
        "        loss_list.append(epoch_loss)\n",
        "        \n",
        "        print('iteration ', e, ' loss ', epoch_loss)\n",
        "        print('          prec rec  f1   acc')\n",
        "         \n",
        "        precision, recall, f_score, classification_accuracy = test_network(net, trainLoader)\n",
        "        train_acc_list.append(classification_accuracy)\n",
        "        print('train set',\"{0:.10f}\".format(precision),\"{0:.10f}\".format(recall),\"{0:.10f}\".format(f_score),\"{0:.10f}\".format(classification_accuracy))\n",
        "        \n",
        "        precision, recall, f_score, classification_accuracy = test_network(net, valLoader)\n",
        "        val_acc_list.append(classification_accuracy)\n",
        "        print('val set  ',\"{0:.10f}\".format(precision),\"{0:.10f}\".format(recall),\"{0:.10f}\".format(f_score),\"{0:.10f}\".format(classification_accuracy))\n",
        "\n",
        "        print()\n",
        "\n",
        "    return net, epochs_list, train_acc_list, val_acc_list, loss_list\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ynVneEWU-Y9u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def createLoaders(train_set, val_set, test_set):\n",
        "    trainLoader = torch.utils.data.DataLoader(train_set, batch_size = 1, shuffle = True, drop_last = True, pin_memory = True)\n",
        "    valLoader = torch.utils.data.DataLoader(val_set, batch_size = 1, shuffle = True, drop_last = True, pin_memory = True)\n",
        "    testLoader = torch.utils.data.DataLoader(test_set, batch_size = 1, shuffle = False, drop_last = True, pin_memory = True)\n",
        "    return trainLoader, valLoader, testLoader\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tDgzhl33pG0D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def drawEpochsAccuracyGraph(x_epochs, y_train, y_val):\n",
        "    plt.plot(x_epochs, y_train, label = \"Train\")\n",
        "    plt.plot(x_epochs, y_val, label = \"Val\")\n",
        "\n",
        "    # naming the axes\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy (%)')\n",
        "    plt.legend()\n",
        "\n",
        "    # title\n",
        "    plt.title('Epochs vs Accuracy')\n",
        "    plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MoHXS22CMcY_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def drawEpochsLossGraph(x_epochs, y_loss):\n",
        "    plt.plot(x_epochs, y_loss, label = \"Loss\")\n",
        "    # naming the axes\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss (%)')\n",
        "    plt.legend()\n",
        "\n",
        "    # title\n",
        "    plt.title('Epochs vs Loss')\n",
        "    plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lpymp969yyCe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# realistic_clean_opseq_folder = '/content/drive/My Drive/After4thYear/Belfast/MSc Cybersec/Research Project/Colab Notebooks/Opseq/Realistic_Clean_Opseq'\n",
        "# realistic_vuln_opseq_folder = '/content/drive/My Drive/After4thYear/Belfast/MSc Cybersec/Research Project/Colab Notebooks/Opseq/Realistic_Vuln_Opseq'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXoIds8Hyr0b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def read_realistic_dataset():\n",
        "#     vuln = []\n",
        "#     clean = []\n",
        "#     clean_opseq_files = []\n",
        "#     vuln_opseq_files = []\n",
        "#     # min_file_len = opt.min_opcode_seq_len #ignore opcode seq files shorter than this\n",
        "#     find_files(clean_opseq_files, dirs=[realistic_clean_opseq_folder], extensions=['.clean'])\n",
        "#     for clean_file_pathname in clean_opseq_files:\n",
        "#         tmp = read_file(clean_file_pathname)\n",
        "#         # if len(tmp) >= min_file_len:\n",
        "#         clean.append(tmp)\n",
        "    \n",
        "#     find_files(vuln_opseq_files, dirs=[realistic_vuln_opseq_folder], extensions=['.vuln'])\n",
        "#     for vuln_file_pathname in vuln_opseq_files:\n",
        "#         tmp = read_file(vuln_file_pathname)\n",
        "#         # if len(tmp) >= min_file_len:\n",
        "#         vuln.append(tmp)\n",
        "    \n",
        "#     # flatten vuln and clean lists\n",
        "#     new_vuln = []\n",
        "#     for x in vuln:\n",
        "#         for y in x:\n",
        "#             new_vuln.append(y)\n",
        "    \n",
        "#     new_clean = []\n",
        "#     for x in clean:\n",
        "#         for y in x:\n",
        "#             new_clean.append(y)\n",
        "#     return new_vuln, new_clean"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ft0arIeXvNzZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def testRealisticSet():\n",
        "#   vuln, clean = read_realistic_dataset()\n",
        "#   vuln_label = 0\n",
        "#   clean_label = 1\n",
        "#   dataset = deepcopy(clean) + deepcopy(vuln) # concatenate original clean and original vuln samples\n",
        "#   trial_dataset = clean + vuln\n",
        "\n",
        "#   #pad with zeroes to make all sequences a standard length of 1000\n",
        "#   for list in trial_dataset:\n",
        "#       if len(list) < 1000:\n",
        "#           list.extend([0] * (1000- len(list)))\n",
        "\n",
        "#   final_vuln_list = []\n",
        "#   for vuln_list in vuln:\n",
        "#       inner_vuln_list = []\n",
        "#       inner_vuln_list.append(vuln_list)\n",
        "#       inner_vuln_list.append(vuln_label)\n",
        "#       final_vuln_list.append(inner_vuln_list)\n",
        "\n",
        "#   final_clean_list = []\n",
        "#   for clean_list in clean:\n",
        "#       inner_clean_list = []\n",
        "#       inner_clean_list.append(clean_list)\n",
        "#       inner_clean_list.append(clean_label)\n",
        "#       final_clean_list.append(inner_clean_list)\n",
        "  \n",
        "#   test_realistic_set = final_clean_list + final_vuln_list\n",
        "#   testRealisticLoader = torch.utils.data.DataLoader(test_realistic_set, batch_size = 1, shuffle = False, drop_last = True) \n",
        "#   return testRealisticLoader"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_rCh-Tc83vq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_detection_set():\n",
        "  detection_opseq_folder = '/content/drive/My Drive/After4thYear/Belfast/MSc Cybersec/Research Project/Colab Notebooks/Opseq/Detection_Opseq'\n",
        "  detection_set = []\n",
        "  detection_opseq_files = []\n",
        "  find_files(detection_opseq_files, dirs=[detection_opseq_folder], extensions=['.txt'])\n",
        "  for detection_file_pathname in detection_opseq_files:\n",
        "      tmp = read_file(detection_file_pathname)\n",
        "      detection_set.append(tmp)\n",
        "\n",
        "  # flatten list\n",
        "  new_detection_set = []\n",
        "  for x in detection_set:\n",
        "      for y in x:\n",
        "          new_detection_set.append(y)\n",
        "  \n",
        "  #pad with zeroes to make all sequences a standard length of 1000\n",
        "  for list in new_detection_set:\n",
        "      if len(list) < 1000:\n",
        "          list.extend([0] * (1000- len(list)))\n",
        "  testDetectionLoader = torch.utils.data.DataLoader(new_detection_set, batch_size = 1, shuffle = False, drop_last = True)       \n",
        "  return testDetectionLoader"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ju85QPkc5ypc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_detection(net, tvtLoader):\n",
        "  predictions = []\n",
        "  \n",
        "  with torch.no_grad():\n",
        "    for i, (opsqs) in enumerate(tvtLoader):\n",
        "        net.eval()\n",
        "        opsqs = torch.stack(opsqs).flatten()\n",
        "        opsqs_tensor = opsqs.type(torch.LongTensor)\n",
        "        output = net(opsqs_tensor.view(-1, 1000))\n",
        "        val,idx = torch.max(output.data,1) # max pool - max over the rows\n",
        "        predictions.append(idx.item())\n",
        "        pdb.set_trace()\n",
        "  for pred_item in predictions:\n",
        "    if (pred_item == 0):\n",
        "      print(\"Alert! Alert! Vulnerability detected\")\n",
        "      print (pred_item)\n",
        "    else: \n",
        "      print(\"Hurray it's clean!\")\n",
        "      print (pred_item)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KoRlwT8G-qtZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def main():\n",
        "    # create_clean_opseq_files() # should only run the first time this code is ran\n",
        "    # print('Created clean opseq')\n",
        "    # create_vuln_opseq_files() # should only run the first time this code is ran\n",
        "    # print('Created vuln opseq')\n",
        "    vuln, clean = read_dataset()\n",
        "\n",
        "    dataset, train_set, val_set, test_set = split_dataset(vuln, clean)\n",
        "    \n",
        "#     # test that the code is actually working by breaking the validation set - accuracy should be around 50%\n",
        "#     for (idx, outer_list) in enumerate(val_set):\n",
        "#         random.shuffle(outer_list[0])\n",
        "        \n",
        "    trainLoader, valLoader, testLoader = createLoaders(train_set, val_set, test_set)\n",
        "#     testRealisticLoader = testRealisticSet()\n",
        "    testDetectionLoader = create_detection_set()\n",
        "    net = VulnerabilityDetectorNetwork()\n",
        "    # net.to(device)\n",
        "    print(net)\n",
        "    print('started training network')\n",
        "    before = time.time()\n",
        "    \n",
        "    net, epochs_list, train_acc_list, val_acc_list, loss_list = train_network(net, trainLoader, valLoader)\n",
        "    print('started testing network')\n",
        "    precision, recall, f_score, classification_accuracy = test_network(net, testLoader)\n",
        "    print('test set',\"{0:.6f}\".format(precision),\"{0:.6f}\".format(recall),\"{0:.6f}\".format(f_score),\"{0:.6f}\".format(classification_accuracy))\n",
        "    print()\n",
        "    \n",
        "#     precision, recall, f_score, classification_accuracy = test_network(net, testRealisticLoader)\n",
        "#     print('test set',\"{0:.6f}\".format(precision),\"{0:.6f}\".format(recall),\"{0:.6f}\".format(f_score),\"{0:.6f}\".format(classification_accuracy))\n",
        "#     print()\n",
        "    test_detection(net, testDetectionLoader)\n",
        "    eval_time = time.time() - before\n",
        "    print (\"Time to run: \", eval_time)\n",
        "    drawEpochsAccuracyGraph(epochs_list, train_acc_list, val_acc_list)\n",
        "    drawEpochsLossGraph(epochs_list, loss_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uWh492Xi-uYS",
        "colab_type": "code",
        "outputId": "1830cfea-208f-4ed8-9dec-d605683b9a06",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        }
      },
      "source": [
        "# print('Running Test Case: Evaluation Function')\n",
        "# test_case_result_eval_function = test_cases.test_evaluation_function(evaluate_network_performance)\n",
        "# print('Test Case Result: ', test_case_result_eval_function)\n",
        "# print()\n",
        "\n",
        "# print('Running Test Case: Splitting Dataset')\n",
        "# test_vuln,test_clean = read_dataset()\n",
        "# test_case_result_split_dataset = test_cases.test_split_dataset(split_dataset, test_vuln, test_clean)\n",
        "# print('Test Case Result: ', test_case_result_split_dataset)\n",
        "# print()\n",
        "\n",
        "# print('Running Test Case: Neural Network')\n",
        "# test_case_result_network = test_cases.test_network(VulnerabilityDetectorNetwork)\n",
        "# print('Test Case Result: ', test_case_result_network)\n",
        "# print()\n",
        "\n",
        "# if test_case_result_eval_function and test_case_result_split_dataset and test_case_result_network:\n",
        "  main()\n",
        "# else:\n",
        "#     print('one or more test cases failed - quitting')\n",
        "#     quit()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "VulnerabilityDetectorNetwork(\n",
            "  (emb1): Embedding(205, 8)\n",
            "  (conv1): Conv2d(1, 64, kernel_size=(8, 8), stride=(1, 1))\n",
            "  (lin1): Linear(in_features=64, out_features=16, bias=True)\n",
            "  (lin2): Linear(in_features=16, out_features=2, bias=True)\n",
            "  (drop_out): Dropout(p=0.5)\n",
            ")\n",
            "started training network\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lme5bgO3qQtZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from __future__ import print_function, with_statement, division\n",
        "# import copy\n",
        "# import os\n",
        "# import torch\n",
        "# from tqdm.autonotebook import tqdm\n",
        "# from torch.optim.lr_scheduler import _LRScheduler\n",
        "# import matplotlib.pyplot as plt\n",
        "# import pdb\n",
        "\n",
        "\n",
        "# class LRFinder(object):\n",
        "#     \"\"\"Learning rate range test.\n",
        "\n",
        "#     The learning rate range test increases the learning rate in a pre-training run\n",
        "#     between two boundaries in a linear or exponential manner. It provides valuable\n",
        "#     information on how well the network can be trained over a range of learning rates\n",
        "#     and what is the optimal learning rate.\n",
        "\n",
        "#     Arguments:\n",
        "#         model (torch.nn.Module): wrapped model.\n",
        "#         optimizer (torch.optim.Optimizer): wrapped optimizer where the defined learning\n",
        "#             is assumed to be the lower boundary of the range test.\n",
        "#         criterion (torch.nn.Module): wrapped loss function.\n",
        "#         device (str or torch.device, optional): a string (\"cpu\" or \"cuda\") with an\n",
        "#             optional ordinal for the device type (e.g. \"cuda:X\", where is the ordinal).\n",
        "#             Alternatively, can be an object representing the device on which the\n",
        "#             computation will take place. Default: None, uses the same device as `model`.\n",
        "#         memory_cache (boolean): if this flag is set to True, `state_dict` of model and\n",
        "#             optimizer will be cached in memory. Otherwise, they will be saved to files\n",
        "#             under the `cache_dir`.\n",
        "#         cache_dir (string): path for storing temporary files. If no path is specified,\n",
        "#             system-wide temporary directory is used.\n",
        "#             Notice that this parameter will be ignored if `memory_cache` is True.\n",
        "\n",
        "#     Example:\n",
        "#         >>> lr_finder = LRFinder(net, optimizer, criterion, device=\"cuda\")\n",
        "#         >>> lr_finder.range_test(dataloader, end_lr=100, num_iter=100)\n",
        "\n",
        "#     Cyclical Learning Rates for Training Neural Networks: https://arxiv.org/abs/1506.01186\n",
        "#     fastai/lr_find: https://github.com/fastai/fastai\n",
        "\n",
        "#     \"\"\"\n",
        "\n",
        "#     def __init__(self, model, optimizer, criterion, device=None, memory_cache=True, cache_dir=None):\n",
        "#         self.model = model\n",
        "#         self.optimizer = optimizer\n",
        "#         self.criterion = criterion\n",
        "#         self.history = {\"lr\": [], \"loss\": []}\n",
        "#         self.best_loss = None\n",
        "#         self.memory_cache = memory_cache\n",
        "#         self.cache_dir = cache_dir\n",
        "\n",
        "#         # Save the original state of the model and optimizer so they can be restored if\n",
        "#         # needed\n",
        "#         self.model_device = next(self.model.parameters()).device\n",
        "#         self.state_cacher = StateCacher(memory_cache, cache_dir=cache_dir)\n",
        "#         self.state_cacher.store('model', self.model.state_dict())\n",
        "#         self.state_cacher.store('optimizer', self.optimizer.state_dict())\n",
        "\n",
        "#         # If device is None, use the same as the model\n",
        "#         if device:\n",
        "#             self.device = device\n",
        "#         else:\n",
        "#             self.device = self.model_device\n",
        "\n",
        "#     def reset(self):\n",
        "#         \"\"\"Restores the model and optimizer to their initial states.\"\"\"\n",
        "#         self.model.load_state_dict(self.state_cacher.retrieve('model'))\n",
        "#         self.optimizer.load_state_dict(self.state_cacher.retrieve('optimizer'))\n",
        "#         self.model.to(self.model_device)\n",
        "\n",
        "#     def range_test(\n",
        "#         self,\n",
        "#         train_loader,\n",
        "#         val_loader=None,\n",
        "#         end_lr=10,\n",
        "#         num_iter=100,\n",
        "#         step_mode=\"exp\",\n",
        "#         smooth_f=0.05,\n",
        "#         diverge_th=5,\n",
        "#     ):\n",
        "#         \"\"\"Performs the learning rate range test.\n",
        "\n",
        "#         Arguments:\n",
        "#             train_loader (torch.utils.data.DataLoader): the training set data laoder.\n",
        "#             val_loader (torch.utils.data.DataLoader, optional): if `None` the range test\n",
        "#                 will only use the training loss. When given a data loader, the model is\n",
        "#                 evaluated after each iteration on that dataset and the evaluation loss\n",
        "#                 is used. Note that in this mode the test takes significantly longer but\n",
        "#                 generally produces more precise results. Default: None.\n",
        "#             end_lr (float, optional): the maximum learning rate to test. Default: 10.\n",
        "#             num_iter (int, optional): the number of iterations over which the test\n",
        "#                 occurs. Default: 100.\n",
        "#             step_mode (str, optional): one of the available learning rate policies,\n",
        "#                 linear or exponential (\"linear\", \"exp\"). Default: \"exp\".\n",
        "#             smooth_f (float, optional): the loss smoothing factor within the [0, 1[\n",
        "#                 interval. Disabled if set to 0, otherwise the loss is smoothed using\n",
        "#                 exponential smoothing. Default: 0.05.\n",
        "#             diverge_th (int, optional): the test is stopped when the loss surpasses the\n",
        "#                 threshold:  diverge_th * best_loss. Default: 5.\n",
        "\n",
        "#         \"\"\"\n",
        "#         # Reset test results\n",
        "#         self.history = {\"lr\": [], \"loss\": []}\n",
        "#         self.best_loss = None\n",
        "\n",
        "#         # Move the model to the proper device\n",
        "#         self.model.to(self.device)\n",
        "\n",
        "#         # Initialize the proper learning rate policy\n",
        "#         if step_mode.lower() == \"exp\":\n",
        "#             lr_schedule = ExponentialLR(self.optimizer, end_lr, num_iter)\n",
        "#         elif step_mode.lower() == \"linear\":\n",
        "#             lr_schedule = LinearLR(self.optimizer, end_lr, num_iter)\n",
        "#         else:\n",
        "#             raise ValueError(\"expected one of (exp, linear), got {}\".format(step_mode))\n",
        "\n",
        "#         if smooth_f < 0 or smooth_f >= 1:\n",
        "#             raise ValueError(\"smooth_f is outside the range [0, 1[\")\n",
        "\n",
        "#         # Create an iterator to get data batch by batch\n",
        "#         iterator = iter(train_loader)\n",
        "#         for iteration in tqdm(range(num_iter)):\n",
        "#             # Get a new set of inputs and labels\n",
        "#             try:\n",
        "#                 inputs, labels = next(iterator)\t\t\t\t\n",
        "\t\t\t\t\n",
        "#             except StopIteration:\n",
        "#                 iterator = iter(train_loader)\n",
        "#                 inputs, labels = next(iterator)\n",
        "\n",
        "#             # Train on batch and retrieve loss\n",
        "#             loss = self._train_batch(inputs, labels)\n",
        "#             if val_loader:\n",
        "#                 loss = self._validate(val_loader)\n",
        "\n",
        "#             # Update the learning rate\n",
        "#             lr_schedule.step()\n",
        "#             self.history[\"lr\"].append(lr_schedule.get_lr()[0])\n",
        "\n",
        "#             # Track the best loss and smooth it if smooth_f is specified\n",
        "#             if iteration == 0:\n",
        "#                 self.best_loss = loss\n",
        "#             else:\n",
        "#                 if smooth_f > 0:\n",
        "#                     loss = smooth_f * loss + (1 - smooth_f) * self.history[\"loss\"][-1]\n",
        "#                 if loss < self.best_loss:\n",
        "#                     self.best_loss = loss\n",
        "\n",
        "#             # Check if the loss has diverged; if it has, stop the test\n",
        "#             self.history[\"loss\"].append(loss)\n",
        "#             if loss > diverge_th * self.best_loss:\n",
        "#                 print(\"Stopping early, the loss has diverged\")\n",
        "#                 break\n",
        "\n",
        "#         print(\"Learning rate search finished. See the graph with {finder_name}.plot()\")\n",
        "\n",
        "#     def _train_batch(self, inputs, labels):\n",
        "#         # Set model to training mode\n",
        "#         self.model.train()\n",
        "# # \t\tinputs_tensor = inputs.type(torch.LongTensor)\n",
        "# # \t\tinputs = torch.stack(inputs_tensor).flatten()\n",
        "# # \t\tlabels = labels.type(torch.LongTensor)\n",
        "# #         pdb.set_trace()\n",
        "        \n",
        "#         # Forward pass\n",
        "#         self.optimizer.zero_grad()\n",
        "#         inputs = torch.stack(inputs).flatten()\n",
        "#         inputs = inputs.type(torch.LongTensor)\n",
        "#         # Move data to the correct device\n",
        "#         inputs = inputs.to(self.device)\n",
        "#         labels = labels.to(self.device)\n",
        "\n",
        "#         outputs = self.model(inputs.view(-1, 1000))\n",
        "#         loss = self.criterion(outputs, labels)\n",
        "\n",
        "#         # Backward pass\n",
        "#         loss.backward()\n",
        "#         self.optimizer.step()\n",
        "\n",
        "#         return loss.item()\n",
        "\n",
        "#     def _validate(self, dataloader):\n",
        "#         # Set model to evaluation mode and disable gradient computation\n",
        "#         running_loss = 0\n",
        "#         self.model.eval()\n",
        "#         with torch.no_grad():\n",
        "#             for inputs, labels in dataloader:\n",
        "#                 inputs = torch.stack(inputs).flatten()\n",
        "#                 inputs = inputs.type(torch.LongTensor)\n",
        "#                 # Move data to the correct device\n",
        "#                 inputs = inputs.to(self.device)\n",
        "#                 labels = labels.to(self.device)\n",
        "\n",
        "#                 # Forward pass and loss computation\n",
        "#                 outputs = self.model(inputs.view(-1, 1000))\n",
        "#                 loss = self.criterion(outputs, labels)\n",
        "#                 running_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "#         return running_loss / len(dataloader.dataset)\n",
        "\n",
        "#     def plot(self, skip_start=10, skip_end=5, log_lr=True):\n",
        "#         \"\"\"Plots the learning rate range test.\n",
        "\n",
        "#         Arguments:\n",
        "#             skip_start (int, optional): number of batches to trim from the start.\n",
        "#                 Default: 10.\n",
        "#             skip_end (int, optional): number of batches to trim from the start.\n",
        "#                 Default: 5.\n",
        "#             log_lr (bool, optional): True to plot the learning rate in a logarithmic\n",
        "#                 scale; otherwise, plotted in a linear scale. Default: True.\n",
        "\n",
        "#         \"\"\"\n",
        "\n",
        "#         if skip_start < 0:\n",
        "#             raise ValueError(\"skip_start cannot be negative\")\n",
        "#         if skip_end < 0:\n",
        "#             raise ValueError(\"skip_end cannot be negative\")\n",
        "\n",
        "#         # Get the data to plot from the history dictionary. Also, handle skip_end=0\n",
        "#         # properly so the behaviour is the expected\n",
        "#         lrs = self.history[\"lr\"]\n",
        "#         losses = self.history[\"loss\"]\n",
        "#         if skip_end == 0:\n",
        "#             lrs = lrs[skip_start:]\n",
        "#             losses = losses[skip_start:]\n",
        "#         else:\n",
        "#             lrs = lrs[skip_start:-skip_end]\n",
        "#             losses = losses[skip_start:-skip_end]\n",
        "\n",
        "#         # Plot loss as a function of the learning rate\n",
        "#         plt.plot(lrs, losses)\n",
        "#         if log_lr:\n",
        "#             plt.xscale(\"log\")\n",
        "#         plt.xlabel(\"Learning rate\")\n",
        "#         plt.ylabel(\"Loss\")\n",
        "#         plt.show()\n",
        "\n",
        "\n",
        "# class LinearLR(_LRScheduler):\n",
        "#     \"\"\"Linearly increases the learning rate between two boundaries over a number of\n",
        "#     iterations.\n",
        "\n",
        "#     Arguments:\n",
        "#         optimizer (torch.optim.Optimizer): wrapped optimizer.\n",
        "#         end_lr (float, optional): the initial learning rate which is the lower\n",
        "#             boundary of the test. Default: 10.\n",
        "#         num_iter (int, optional): the number of iterations over which the test\n",
        "#             occurs. Default: 100.\n",
        "#         last_epoch (int): the index of last epoch. Default: -1.\n",
        "\n",
        "#     \"\"\"\n",
        "\n",
        "#     def __init__(self, optimizer, end_lr, num_iter, last_epoch=-1):\n",
        "#         self.end_lr = end_lr\n",
        "#         self.num_iter = num_iter\n",
        "#         super(LinearLR, self).__init__(optimizer, last_epoch)\n",
        "\n",
        "#     def get_lr(self):\n",
        "#         curr_iter = self.last_epoch + 1\n",
        "#         r = curr_iter / self.num_iter\n",
        "#         return [base_lr + r * (self.end_lr - base_lr) for base_lr in self.base_lrs]\n",
        "\n",
        "\n",
        "# class ExponentialLR(_LRScheduler):\n",
        "#     \"\"\"Exponentially increases the learning rate between two boundaries over a number of\n",
        "#     iterations.\n",
        "\n",
        "#     Arguments:\n",
        "#         optimizer (torch.optim.Optimizer): wrapped optimizer.\n",
        "#         end_lr (float, optional): the initial learning rate which is the lower\n",
        "#             boundary of the test. Default: 10.\n",
        "#         num_iter (int, optional): the number of iterations over which the test\n",
        "#             occurs. Default: 100.\n",
        "#         last_epoch (int): the index of last epoch. Default: -1.\n",
        "\n",
        "#     \"\"\"\n",
        "\n",
        "#     def __init__(self, optimizer, end_lr, num_iter, last_epoch=-1):\n",
        "#         self.end_lr = end_lr\n",
        "#         self.num_iter = num_iter\n",
        "#         super(ExponentialLR, self).__init__(optimizer, last_epoch)\n",
        "\n",
        "#     def get_lr(self):\n",
        "#         curr_iter = self.last_epoch + 1\n",
        "#         r = curr_iter / self.num_iter\n",
        "#         return [base_lr * (self.end_lr / base_lr) ** r for base_lr in self.base_lrs]\n",
        "\n",
        "\n",
        "# class StateCacher(object):\n",
        "#     def __init__(self, in_memory, cache_dir=None):\n",
        "#         self.in_memory = in_memory\n",
        "#         self.cache_dir = cache_dir\n",
        "\n",
        "#         if self.cache_dir is None:\n",
        "#             import tempfile\n",
        "#             self.cache_dir = tempfile.gettempdir()\n",
        "#         else:\n",
        "#             if not os.path.isdir(self.cache_dir):\n",
        "#                 raise ValueError('Given `cache_dir` is not a valid directory.')\n",
        "\n",
        "#         self.cached = {}\n",
        "\n",
        "#     def store(self, key, state_dict):\n",
        "#         if self.in_memory:\n",
        "#             self.cached.update({key: copy.deepcopy(state_dict)})\n",
        "#         else:\n",
        "#             fn = os.path.join(self.cache_dir, 'state_{}_{}.pt'.format(key, id(self)))\n",
        "#             self.cached.update({key: fn})\n",
        "#             torch.save(state_dict, fn)\n",
        "\n",
        "#     def retrieve(self, key):\n",
        "#         if key not in self.cached:\n",
        "#             raise KeyError('Target {} was not cached.'.format(key))\n",
        "\n",
        "#         if self.in_memory:\n",
        "#             return self.cached.get(key)\n",
        "#         else:\n",
        "#             fn = self.cached.get(key)\n",
        "#             if not os.path.exists(fn):\n",
        "#                 raise RuntimeError('Failed to load state in {}. File does not exist anymore.'.format(fn))\n",
        "#             state_dict = torch.load(fn, map_location=lambda storage, location: storage)\n",
        "#             return state_dict\n",
        "\n",
        "#     def __del__(self):\n",
        "#         \"\"\"Check whether there are unused cached files existing in `cache_dir` before\n",
        "#         this instance being destroyed.\"\"\"\n",
        "#         if self.in_memory:\n",
        "#             return\n",
        "\n",
        "#         for k in self.cached:\n",
        "#             if os.path.exists(self.cached[k]):\n",
        "#                 os.remove(self.cached[k])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PbjXGcC2b6AQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# vuln, clean = read_dataset()\n",
        "\n",
        "# dataset, train_set, val_set, test_set = split_dataset(vuln, clean)\n",
        "# trainLoader, valLoader, testLoader = createLoaders(train_set, val_set, test_set)\n",
        "# net = VulnerabilityDetectorNetwork()\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "# optimizer = optim.Adam(net.parameters(), lr=1e-6)\n",
        "# lr_finder = LRFinder(net, optimizer, criterion)\n",
        "# lr_finder.range_test(trainLoader, end_lr=1, num_iter=100, step_mode=\"exp\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KLncqQBscliv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# lr_finder.plot()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OMFBT0Focnz-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# lr_finder.reset()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vRme160S5_pz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# lr_finder.range_test(trainLoader, val_loader=valLoader, end_lr=1, num_iter=100, step_mode=\"exp\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mHdfqKH36BwX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# lr_finder.plot(skip_end=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMprFiza6CWX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# lr_finder.reset()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}