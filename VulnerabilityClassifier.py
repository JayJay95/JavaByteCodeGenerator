import os
import re
import fnmatch
import argparse
import math
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.utils.data
from torch.autograd import Variable
import torch.optim as optim
import numpy as np
import test_cases
from copy import deepcopy
import pdb
import time
# import matplotlib.pyplot as plt

clean_opcode_folder = 'D:/After4thYear/MSc Applied Cyber Security/Research Project/tests/JavaByteCodeGenerator/Opcodes/Clean_Opcodes'
vuln_opcode_folder = 'D:/After4thYear/MSc Applied Cyber Security/Research Project/tests/JavaByteCodeGenerator/Opcodes/Vuln_Opcodes'
CWE_folders = 'D:/After4thYear/MSc Applied Cyber Security/Research Project/tests/Juliet_Test_Suite_v1.3_for_Java/Java/src/testcases'
clean_opseq_folder = 'D:/After4thYear/MSc Applied Cyber Security/Research Project/tests/JavaByteCodeGenerator/Opseq/Clean_Opseq'
vuln_opseq_folder = 'D:/After4thYear/MSc Applied Cyber Security/Research Project/tests/JavaByteCodeGenerator/Opseq/Vuln_Opseq'
dataset_root = 'D:/After4thYear/MSc Applied Cyber Security/Research Project/tests/JavaByteCodeGenerator/Opseq/'

parser = argparse.ArgumentParser(description='Vulnerability Classifier')
# parser.add_argument('--max_opcode_seq_len', action='store', type=int, help='use different versions of network', default=8192)
# parser.add_argument('--min_opcode_seq_len', action='store', type=int, help='use different versions of network', default=32)
parser.add_argument('--lr', action='store', type=float, help='use different versions of network', default=1e-3)
parser.add_argument('--epochs', action='store', type=int, help='use different versions of network', default=10)
opt = parser.parse_args()
print(opt)

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

def find_files(files, dirs=[], extensions=[]): # recursively find files in directories
    new_dirs = []
    for d in dirs:
        try:
            new_dirs += [ os.path.join(d, f) for f in os.listdir(d)] # check in all directories except testcasesupport 
        except OSError:
            if os.path.splitext(d)[1] in extensions:
                files.append(d)

    if new_dirs:
        find_files(files, new_dirs, extensions)
    else:
        return  

def get_CWE_folder_names(dirs=[]):
    new_dirs = []
    unique_cwe_list = []
    for d in dirs:
        new_dirs += [ os.path.join(d, f) for f in os.listdir(d)] 
    for nd in new_dirs:
        match = re.search(r'(CWE\d*)',nd)
        if match is not None:
            unique_cwe_list.append(match.group(1))
    return unique_cwe_list


def create_clean_opseq_files():
    CWE_names_list = []
    clean_opcode_files = []
    
    CWE_names_list = get_CWE_folder_names(dirs=[CWE_folders])
        
    find_files(clean_opcode_files, dirs=[clean_opcode_folder], extensions=['.txt']) # find all class files in the directory 
    for name in CWE_names_list:
        for file_pathname in clean_opcode_files:
            opseq_file = clean_opseq_folder + '/' + name + '.clean'
            if fnmatch.fnmatch(file_pathname, "*"+name+"*"):
                read_clean_opcode_file = open(file_pathname, "r")
                file_to_string = read_clean_opcode_file.read().replace("\n", "")
                read_clean_opcode_file.close()
                with open(opseq_file, "a") as f1:
                    f1.write(file_to_string + '\n')
    
    
def create_vuln_opseq_files():
    CWE_names_list = []
    vuln_opcode_files = []
    
    CWE_names_list = get_CWE_folder_names(dirs=[CWE_folders])
        
    find_files(vuln_opcode_files, dirs=[vuln_opcode_folder], extensions=['.txt']) # find all class files in the directory 
    for name in CWE_names_list:
        for file_pathname in vuln_opcode_files:        
            opseq_file = vuln_opseq_folder + '/' + name + '.vuln'
            if fnmatch.fnmatch(file_pathname, "*"+name+"*"):
                read_vuln_opcode_file = open(file_pathname, "r")
                file_to_string = read_vuln_opcode_file.read().replace("\n", "")
                read_vuln_opcode_file.close()
                
                with open(opseq_file, "a") as f1:
                    f1.write(file_to_string + '\n')
                    
def read_file(filename):
    opcode_count = 0
    line_list = []
    with open(filename, mode='rt', encoding='utf8') as f:
        content = f.readlines()        
    for line in content:
        opcode_seq = []     
        for c in range(0, len(line) - 1, 2):
            #print(line[c:(c+2)],int(line[c:(c+2)], 16))
            opcode_seq.append(int(line[c:(c+2)], 16) + 1) # add one here so that the zero'th embedding is reserved for 'blank' i.e. no instruction whatsoever not even no-op			
            opcode_count += 1
            # to save training time we only read 
            # the first opt.max_opcode_seq_len opcodes of each file
            
            # if opcode_count >= opt.max_opcode_seq_len:
            #     return opcode_seq
        line_list.append(opcode_seq)
    return line_list


def read_dataset():
    vuln = []
    clean = []
    clean_opseq_files = []
    vuln_opseq_files = []
    # min_file_len = opt.min_opcode_seq_len #ignore opcode seq files shorter than this
    find_files(clean_opseq_files, dirs=[clean_opseq_folder], extensions=['.clean'])
    for clean_file_pathname in clean_opseq_files:
        tmp = read_file(clean_file_pathname)
        # if len(tmp) >= min_file_len:
        clean.append(tmp)
    
    find_files(vuln_opseq_files, dirs=[vuln_opseq_folder], extensions=['.vuln'])
    for vuln_file_pathname in vuln_opseq_files:
        tmp = read_file(vuln_file_pathname)
        # if len(tmp) >= min_file_len:
        vuln.append(tmp)
    
    # flatten vuln and clean lists
    new_vuln = []
    for x in vuln:
        for y in x:
            new_vuln.append(y)
    
    new_clean = []
    for x in clean:
        for y in x:
            new_clean.append(y)
                   
    return new_vuln, new_clean

def split_dataset(vuln, clean):
    #split the dataset into train, val, test sets
    #return the concatenated dataset and
    #indicies pointing to the train,val,test samples

    vuln_label = 0
    clean_label = 1
    dataset = deepcopy(clean) + deepcopy(vuln) # concatenate original clean and original vuln samples
    trial_dataset = clean + vuln
    
    #pad with zeroes to make all sequences a standard length of 1000
    for list in trial_dataset:
        if len(list) < 1000:
            list.extend([0] * (1000- len(list)))
    
    final_vuln_list = []
    for vuln_list in vuln:
        inner_vuln_list = []
        inner_vuln_list.append(vuln_list)
        inner_vuln_list.append(vuln_label)
        final_vuln_list.append(inner_vuln_list)
             
    final_clean_list = []
    for clean_list in clean:
        inner_clean_list = []
        inner_clean_list.append(clean_list)
        inner_clean_list.append(clean_label)
        final_clean_list.append(inner_clean_list)
    
    # split vuln samples randomly
    first_vuln_split = int(0.8 * len(final_vuln_list))
    second_vuln_split = int(0.1 * len(final_vuln_list))
    third_vuln_split = int(len(final_vuln_list) - (first_vuln_split + second_vuln_split))
    vuln_training_dataset, vuln_validation_dataset, vuln_testing_dataset = torch.utils.data.random_split(final_vuln_list, [first_vuln_split, second_vuln_split, third_vuln_split])

    # split clean samples randomly
    first_clean_split = int(0.8 * len(final_clean_list))
    second_clean_split = int(0.1*len(final_clean_list))
    third_clean_split = int(len(final_clean_list) - (first_clean_split + second_clean_split))
    clean_training_dataset, clean_validation_dataset, clean_testing_dataset = torch.utils.data.random_split(final_clean_list, [first_clean_split, second_clean_split, third_clean_split])

    # merge both vuln & clean training sets
    training_set = clean_training_dataset + vuln_training_dataset

    # get indices and labels from training set
    train_inds=[]
    train_labels=[]
    training_counter = 0
    
    for list in training_set:
        train_inds.append(training_counter)
        training_counter+=1
        train_labels.append(list[1])
        
    # merge both vuln & clean validation sets
    validation_set = clean_validation_dataset + vuln_validation_dataset

    # get indices and labels from validation set
    val_inds = []
    val_labels = []
    training_set_length = len(training_set)
    for list in validation_set:
        val_inds.append(training_set_length)
        training_set_length += 1
        val_labels.append(list[1])

    # merge both vuln & clean testing sets
    testing_set = clean_testing_dataset + vuln_testing_dataset

    # get indices and labels from testing set
    test_inds = []
    test_labels = []
    validation_set_length = len(training_set) + len(validation_set)
    for list in testing_set:
        test_inds.append(validation_set_length)
        validation_set_length += 1
        test_labels.append(list[1])
    
    return trial_dataset, training_set, validation_set, testing_set

def evaluate_network_performance(predictions, ground_truth):
    # given an array with the predicted values and the correct values
    # calculate the precision, recall and f-score
    # pdb.set_trace()
    cm = get_confusion_matrix(predictions, ground_truth)
    TP = cm[0][0]
    FP = cm[0][1]
    FN = cm[1][0]
    TN = cm[1][1]

    precision = TP/(TP+FP)
    recall = TP/(TP+FN)
    classification_accuracy = (TP+TN) / (TP + TN + FP + FN)
    f_score = 2 * ((precision*recall)/(precision + recall))

    return precision, recall, f_score, classification_accuracy

class VulnerabilityDetectorNetwork(nn.Module):
    def __init__(self):
        super(VulnerabilityDetectorNetwork, self).__init__()

        self.num_of_embeddings = 205
        self.embedding_dimension = 8
        self.channels_in = 1
        self.channels_out = 64
        self.hidden_nodes = 16
        self.kernel_height_dimension = 8
        self.kernel_width_dimension = 8
        self.padding_height_dimension = 0
        self.padding_width_dimension = 0
        self.features_out = 2

        self.emb1 = nn.Embedding(self.num_of_embeddings, self.embedding_dimension)
        self.conv1 = nn.Conv2d(self.channels_in, self.channels_out, kernel_size=(self.kernel_height_dimension, self.kernel_width_dimension),
            padding=(self.padding_height_dimension, self.padding_width_dimension))
        self.lin1 = nn.Linear(self.channels_out, self.hidden_nodes)
        self.lin2 = nn.Linear(self.hidden_nodes, self.features_out)
        self.drop_out = nn.Dropout(0.5)

    def forward(self, x):
#         print(x.shape)
        x = self.emb1(x)
#         print(x.shape)        
        x = x.unsqueeze(1)
#         x = x.unsqueeze(0)
#         print(x.shape)
        x = self.conv1(x)
#         print(x.shape)
        x = F.relu(x)
#         print(x.shape)        
        x = torch.max(x,2)[0]
#         print(x.shape)
        x = self.drop_out(x)
#         print(x.shape)
        x = x.squeeze(2)
#         print(x.shape)
        x = self.lin1(x)
#         print(x.shape)
        x = self.lin2(x)
#         print(x.shape)
        return x

def get_confusion_matrix(preds, truth):
    flat_truth = torch.stack(truth).flatten()
    flat_preds = torch.stack(preds).flatten()
    K = len(np.unique(flat_truth)) # Number of classes 
    result = np.zeros((K, K))
    for i in range(len(flat_truth)):
        result[flat_preds[i]][flat_truth[i]] += 1
    confusion_matrix = result
    return confusion_matrix

def test_network(net, tvtLoader):
    
    criterion = nn.CrossEntropyLoss()

    test_loss = 0
    predictions = []
    labels = []

    with torch.no_grad():
#         for i in range(len(inds)):
        for i, (opsqs, lbls) in enumerate(tvtLoader):
            net.eval()
            opsqs = torch.stack(opsqs).flatten()
            opsqs_tensor = opsqs.type(torch.LongTensor)
            lbls = lbls.type(torch.LongTensor)
            opsqs_tensor, lbls = opsqs_tensor.to(device), lbls.to(device)
                
            output = net(opsqs_tensor.view(-1, 1000))
            val, idx = torch.max(output.data, 1) # max pool - max over the rows
#             predictions.append(idx.item())
            predictions.append(idx)
            labels.append(lbls)

            test_loss = criterion(output, lbls).item()

        precision, recall, f_score, classification_accuracy = evaluate_network_performance(predictions, labels)

    return precision, recall, f_score, classification_accuracy

def train_network(net, trainLoader, valLoader):
    epochs_list = []
    train_acc_list = []
    val_acc_list = []
    
    optimizer = optim.Adam(net.parameters(), lr=opt.lr)
    criterion = nn.CrossEntropyLoss()

    num_training_epochs = opt.epochs

    for e in range(num_training_epochs):
        running_loss = 0

        for i, (opseqs, labels) in enumerate(trainLoader):
            net.train() # set network into training mode
            optimizer.zero_grad() #reset the optimizer before every loop
            
            opseqs = torch.stack(opseqs).flatten()
            opseqs_tensor = opseqs.type(torch.LongTensor)
            labels = labels.type(torch.LongTensor)
            opseqs_tensor, labels = opseqs_tensor.to(device), labels.to(device)   
            # pdb.set_trace()
            net_output = net(opseqs_tensor.view(-1, 1000)) # forward pass of current training sample through the network
#             net_output = net(opseqs_tensor.unsqueeze(0))
            loss = criterion(net_output, labels) # how close network is to correct answer
            running_loss += loss.item()
            
            loss.backward() # back propagation step for whole net
            optimizer.step() # call optimizer to update network's parameters

        epoch_loss = running_loss/len(trainLoader)
        epochs_list.append(e)
        
        print('iteration ', e, ' loss ', epoch_loss)
        print('          prec rec  f1   acc')
         
        precision, recall, f_score, classification_accuracy = test_network(net, trainLoader)
        train_acc_list.append(classification_accuracy)
        print('train set',"{0:.2f}".format(precision),"{0:.2f}".format(recall),"{0:.2f}".format(f_score),"{0:.2f}".format(classification_accuracy))
        
        precision, recall, f_score, classification_accuracy = test_network(net, valLoader)
        val_acc_list.append(classification_accuracy)
        print('val set  ',"{0:.2f}".format(precision),"{0:.2f}".format(recall),"{0:.2f}".format(f_score),"{0:.2f}".format(classification_accuracy))

        print()

    return net, epochs_list, train_acc_list, val_acc_list

def createLoaders(train_set, val_set, test_set):
    trainLoader = torch.utils.data.DataLoader(train_set, batch_size = 1, shuffle = True, drop_last = True)
    valLoader = torch.utils.data.DataLoader(val_set, batch_size = 1, shuffle = True, drop_last = True)
    testLoader = torch.utils.data.DataLoader(test_set, batch_size = 1, shuffle = False, drop_last = True)
    return trainLoader, valLoader, testLoader

# def drawEpochsAccuracyGraph(x_epochs, y_train, y_val):
#     plt.plot(x_epochs, y_train, label = "Train")
#     plt.plot(x_epochs, y_val, label = "Val")

#     # naming the axes
#     plt.xlabel('Epochs')
#     plt.ylabel('Accuracy (%)')
#     plt.legend()

#     # title
#     plt.title('Epochs vs Accuracy')
#     plt.show()
    
def main():
    # create_clean_opseq_files() # should only run the first time this code is ran
    # print('Created clean opseq')
    # create_vuln_opseq_files() # should only run the first time this code is ran
    # print('Created vuln opseq')
    vuln, clean = read_dataset()

    dataset, train_set, val_set, test_set = split_dataset(vuln, clean)   
    trainLoader, valLoader, testLoader = createLoaders(train_set, val_set, test_set)
    net = VulnerabilityDetectorNetwork()
    net.to(device)
    print(net)
    print('started training network')
    before = time.time()
    
    net, epochs_list, train_acc_list, val_acc_list = train_network(net, trainLoader, valLoader)
    print('started testing network')
    precision, recall, f_score, classification_accuracy = test_network(net, testLoader)
    print('test set',"{0:.2f}".format(precision),"{0:.2f}".format(recall),"{0:.2f}".format(f_score),"{0:.2f}".format(classification_accuracy))
    print()
    eval_time = time.time() - before
    print ("Time to run: ", eval_time)
    # drawEpochsAccuracyGraph(epochs_list, train_acc_list, val_acc_list)

    
print('Running Test Case: Evaluation Function')
test_case_result_eval_function = test_cases.test_evaluation_function(evaluate_network_performance)
print('Test Case Result: ', test_case_result_eval_function)
print()

print('Running Test Case: Splitting Dataset')
test_vuln,test_clean = read_dataset()
test_case_result_split_dataset = test_cases.test_split_dataset(split_dataset, test_vuln, test_clean)
print('Test Case Result: ', test_case_result_split_dataset)
print()

print('Running Test Case: Neural Network')
test_case_result_network = test_cases.test_network(VulnerabilityDetectorNetwork)
print('Test Case Result: ', test_case_result_network)
print()

if test_case_result_eval_function and test_case_result_split_dataset and test_case_result_network:
# if test_case_result_eval_function and test_case_result_split_dataset:
    main()
else:
    print('one or more test cases failed - quitting')
    quit()
