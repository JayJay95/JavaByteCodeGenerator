import os
import re
import fnmatch
import argparse
import math
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.utils.data
from torch.autograd import Variable
import torch.optim as optim
import numpy as np
import test_cases
from copy import deepcopy
import pdb

clean_opcode_folder = 'D:/After4thYear/MSc Applied Cyber Security/Research Project/tests/JavaByteCodeGenerator/Opcodes/Clean_Opcodes'
vuln_opcode_folder = 'D:/After4thYear/MSc Applied Cyber Security/Research Project/tests/JavaByteCodeGenerator/Opcodes/Vuln_Opcodes'
CWE_folders = 'D:/After4thYear/MSc Applied Cyber Security/Research Project/tests/Juliet_Test_Suite_v1.3_for_Java/Java/src/testcases'
clean_opseq_folder = 'D:/After4thYear/MSc Applied Cyber Security/Research Project/tests/JavaByteCodeGenerator/Opseq/Clean_Opseq'
vuln_opseq_folder = 'D:/After4thYear/MSc Applied Cyber Security/Research Project/tests/JavaByteCodeGenerator/Opseq/Vuln_Opseq'
dataset_root = 'D:/After4thYear/MSc Applied Cyber Security/Research Project/tests/JavaByteCodeGenerator/Opseq/'

parser = argparse.ArgumentParser(description='Vulnerability Classifier')
# parser.add_argument('--max_opcode_seq_len', action='store', type=int, help='use different versions of network', default=8192)
# parser.add_argument('--min_opcode_seq_len', action='store', type=int, help='use different versions of network', default=32)
parser.add_argument('--lr', action='store', type=float, help='use different versions of network', default=1e-3)
parser.add_argument('--epochs', action='store', type=int, help='use different versions of network', default=10)
opt = parser.parse_args()
print(opt)

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

def find_files(files, dirs=[], extensions=[]): # recursively find files in directories
    new_dirs = []
    for d in dirs:
        try:
            new_dirs += [ os.path.join(d, f) for f in os.listdir(d)] # check in all directories except testcasesupport 
        except OSError:
            if os.path.splitext(d)[1] in extensions:
                files.append(d)

    if new_dirs:
        find_files(files, new_dirs, extensions)
    else:
        return  

def get_CWE_folder_names(dirs=[]):
    new_dirs = []
    unique_cwe_list = []
    for d in dirs:
        new_dirs += [ os.path.join(d, f) for f in os.listdir(d)] 
    for nd in new_dirs:
        match = re.search(r'(CWE\d*)',nd)
        if match is not None:
            unique_cwe_list.append(match.group(1))
    return unique_cwe_list


def create_clean_opseq_files():
    CWE_names_list = []
    clean_opcode_files = []
    
    CWE_names_list = get_CWE_folder_names(dirs=[CWE_folders])
        
    find_files(clean_opcode_files, dirs=[clean_opcode_folder], extensions=['.txt']) # find all class files in the directory 
    for name in CWE_names_list:
        for file_pathname in clean_opcode_files:
            opseq_file = clean_opseq_folder + '/' + name + '.clean'
            if fnmatch.fnmatch(file_pathname, "*"+name+"*"):
                read_clean_opcode_file = open(file_pathname, "r")
                file_to_string = read_clean_opcode_file.read().replace("\n", "")
                read_clean_opcode_file.close()
                with open(opseq_file, "a") as f1:
                    f1.write(file_to_string + '\n')
    
    
def create_vuln_opseq_files():
    CWE_names_list = []
    vuln_opcode_files = []
    
    CWE_names_list = get_CWE_folder_names(dirs=[CWE_folders])
        
    find_files(vuln_opcode_files, dirs=[vuln_opcode_folder], extensions=['.txt']) # find all class files in the directory 
    for name in CWE_names_list:
        for file_pathname in vuln_opcode_files:        
            opseq_file = vuln_opseq_folder + '/' + name + '.vuln'
            if fnmatch.fnmatch(file_pathname, "*"+name+"*"):
                read_vuln_opcode_file = open(file_pathname, "r")
                file_to_string = read_vuln_opcode_file.read().replace("\n", "")
                read_vuln_opcode_file.close()
                
                with open(opseq_file, "a") as f1:
                    f1.write(file_to_string + '\n')
                    
def read_file(filename):
    opcode_count = 0
    line_list = []
    with open(filename, mode='rt', encoding='utf8') as f:
        content = f.readlines()        
    for line in content:
        opcode_seq = []     
        for c in range(0, len(line) - 1, 2):
            #print(line[c:(c+2)],int(line[c:(c+2)], 16))
            opcode_seq.append(int(line[c:(c+2)], 16) + 1) # add one here so that the zero'th embedding is reserved for 'blank' i.e. no instruction whatsoever not even no-op			
            opcode_count += 1
            # to save training time we only read 
            # the first opt.max_opcode_seq_len opcodes of each file
            
            # if opcode_count >= opt.max_opcode_seq_len:
            #     return opcode_seq
        line_list.append(opcode_seq)
    return line_list


def read_dataset():
    vuln = []
    clean = []
    clean_opseq_files = []
    vuln_opseq_files = []
    # min_file_len = opt.min_opcode_seq_len #ignore opcode seq files shorter than this
    find_files(clean_opseq_files, dirs=[clean_opseq_folder], extensions=['.clean'])
    for clean_file_pathname in clean_opseq_files:
        tmp = read_file(clean_file_pathname)
        # if len(tmp) >= min_file_len:
        clean.append(tmp)
    
    find_files(vuln_opseq_files, dirs=[vuln_opseq_folder], extensions=['.vuln'])
    for vuln_file_pathname in vuln_opseq_files:
        tmp = read_file(vuln_file_pathname)
        # if len(tmp) >= min_file_len:
        vuln.append(tmp)
    
    # flatten vuln and clean lists
    new_vuln = []
    for x in vuln:
        for y in x:
            new_vuln.append(y)
    
    new_clean = []
    for x in clean:
        for y in x:
            new_clean.append(y)
            
    return new_vuln, new_clean

def split_dataset(vuln, clean):
    #split the dataset into train, val, test sets
    #return the concatenated dataset and
    #indicies pointing to the train,val,test samples

    vuln_label = 0
    clean_label = 1
    dataset = deepcopy(clean) + deepcopy(vuln) # concatenate original clean and original vuln samples
    
    final_vuln_list = []
    for vuln_list in vuln:
        inner_vuln_list = []
        inner_vuln_list.append(vuln_list)
        inner_vuln_list.append(vuln_label)
        final_vuln_list.append(inner_vuln_list)
             
    final_clean_list = []
    for clean_list in clean:
        inner_clean_list = []
        inner_clean_list.append(clean_list)
        inner_clean_list.append(clean_label)
        final_clean_list.append(inner_clean_list)
    
    # split vuln samples randomly
    first_vuln_split = int(0.8 * len(final_vuln_list))
    second_vuln_split = int(0.1 * len(final_vuln_list))
    third_vuln_split = int(len(final_vuln_list) - (first_vuln_split + second_vuln_split))
    vuln_training_dataset, vuln_validation_dataset, vuln_testing_dataset = torch.utils.data.random_split(final_vuln_list, [first_vuln_split, second_vuln_split, third_vuln_split])

    # split clean samples randomly
    first_clean_split = int(0.8 * len(final_clean_list))
    second_clean_split = int(0.1*len(final_clean_list))
    third_clean_split = int(len(final_clean_list) - (first_clean_split + second_clean_split))
    clean_training_dataset, clean_validation_dataset, clean_testing_dataset = torch.utils.data.random_split(final_clean_list, [first_clean_split, second_clean_split, third_clean_split])

    # merge both vuln & clean training sets
    training_set = clean_training_dataset + vuln_training_dataset

    # get indices and labels from training set
    train_inds=[]
    train_labels=[]
    training_counter = 0
    
    for list in training_set:
        train_inds.append(training_counter)
        training_counter+=1
        train_labels.append(list[1])
        
    # merge both vuln & clean validation sets
    validation_set = clean_validation_dataset + vuln_validation_dataset

    # get indices and labels from validation set
    val_inds = []
    val_labels = []
    training_set_length = len(training_set)
    for list in validation_set:
        val_inds.append(training_set_length)
        training_set_length += 1
        val_labels.append(list[1])

    # merge both vuln & clean testing sets
    testing_set = clean_testing_dataset + vuln_testing_dataset

    # get indices and labels from testing set
    test_inds = []
    test_labels = []
    validation_set_length = len(training_set) + len(validation_set)
    for list in testing_set:
        test_inds.append(validation_set_length)
        validation_set_length += 1
        test_labels.append(list[1])
        
    return dataset, (train_inds, train_labels), (val_inds, val_labels), (test_inds, test_labels)

class VulnerabilityDetectorNetwork(nn.Module):
    def __init__(self):
        super(VulnerabilityDetectorNetwork, self).__init__()

        self.num_of_embeddings = 256
        self.embedding_dimension = 8
        self.channels_in = 1
        self.channels_out = 64
        self.kernel_height_dimension = 1
        self.kernel_width_dimension = 8
        self.padding_height_dimension = 0
        self.padding_width_dimension = 0
        self.features_out = 2

        self.emb1 = nn.Embedding(self.num_of_embeddings, self.embedding_dimension)
        self.conv1 = nn.Conv2d(self.channels_in, self.channels_out, kernel_size=(self.kernel_height_dimension, self.kernel_width_dimension),
            padding=(self.padding_height_dimension, self.padding_width_dimension))
        self.lin1 = nn.Linear(self.channels_out, self.features_out)


    def forward(self,x):
        x = self.emb1(x)
        x = x.unsqueeze(0)
        x = self.conv1(x)
        x = F.relu(x)
        x = torch.max(x,2)[0]
        x = x.squeeze(2)
        x = self.lin1(x)
       
        return x

def evaluate_network_performance(predictions, ground_truth):
    # given an array with the predicted values and the correct values
    # calculate the precision, recall and f-score
    cm = get_confusion_matrix(predictions, ground_truth)
    TP = cm[0][0]
    FP = cm[0][1]
    FN = cm[1][0]
    TN = cm[1][1]

    precision = TP/(TP+FP)
    recall = TP/(TP+FN)
    classification_accuracy = (TP+TN) / (TP + TN + FP + FN)
    f_score = 2 * ((precision*recall)/(precision + recall))

    return precision, recall, f_score, classification_accuracy

def get_confusion_matrix(preds, truth):
    K = len(np.unique(truth)) # Number of classes 
    result = np.zeros((K, K))
    for i in range(len(truth)):
        result[preds[i]][truth[i]] += 1
    confusion_matrix = result
    return confusion_matrix

def test_network(net, dataset, tvt_set):
    # given a dataset and network compute the accuracy over the data_set

    inds = tvt_set[0]
    labels = tvt_set[1]

    criterion = nn.CrossEntropyLoss()

    test_loss = 0
    predictions = []

    with torch.no_grad():
        for i in range(len(inds)):

            net.eval()

            example = torch.LongTensor(dataset[inds[i]]).unsqueeze(0)
            target = torch.LongTensor([labels[i]])

            example, target = example.to(device), target.to(device)

            output = net(example)
            val,idx = torch.max(output,1)
            predictions.append(idx.item())

            test_loss = criterion(output, target).item()

        precision, recall, f_score, classification_accuracy = evaluate_network_performance(predictions, labels)

    return precision, recall, f_score, classification_accuracy

def train_network(net, dataset, train_set, val_set):
	
    train_inds = train_set[0]
    train_labels = train_set[1]

    optimizer = optim.SGD(net.parameters(), lr=opt.lr)
    criterion = nn.CrossEntropyLoss()

    num_training_epochs = opt.epochs

    for e in range(num_training_epochs):
        randperm = torch.randperm(len(train_inds))
        epoch_loss = 0
        for i in range(len(train_inds)):

            net.train()
            optimizer.zero_grad()

            example = torch.LongTensor(dataset[train_inds[randperm[i]]]).unsqueeze(0)
            target = torch.LongTensor([train_labels[randperm[i]]])

            example, target = example.to(device), target.to(device)

            net_output = net(example)
            loss = criterion(net_output, target)
            epoch_loss += loss.item()

            loss.backward()
            optimizer.step()


        print('iteration ', e, 'loss ', epoch_loss)
        print('          prec rec  f1   acc')

        precision, recall, f_score, classification_accuracy = test_network(net, dataset, val_set)
        print('val set  ',"{0:.2f}".format(precision),"{0:.2f}".format(recall),"{0:.2f}".format(f_score),"{0:.2f}".format(classification_accuracy))

        precision, recall, f_score, classification_accuracy = test_network(net, dataset, train_set)
        print('train set',"{0:.2f}".format(precision),"{0:.2f}".format(recall),"{0:.2f}".format(f_score),"{0:.2f}".format(classification_accuracy))
        print()

    return net   

def main():
    # create_clean_opseq_files() # should only run the first time this code is ran
    # print('Created clean opseq')
    # create_vuln_opseq_files() # should only run the first time this code is ran
    # print('Created vuln opseq')
    vuln, clean = read_dataset()

    dataset, train_set, val_set, test_set = split_dataset(vuln, clean)
    net = VulnerabilityDetectorNetwork()
    net.to(device)
    print(net)
    print('started training network')
    net = train_network(net, dataset, train_set, val_set)

print('Running Test Case: Evaluation Function')
test_case_result_eval_function = test_cases.test_evaluation_function(evaluate_network_performance)
print('Test Case Result: ', test_case_result_eval_function)
print()

print('Running Test Case: Splitting Dataset')
test_vuln,test_clean = read_dataset()
test_case_result_split_dataset = test_cases.test_split_dataset(split_dataset, test_vuln, test_clean)
print('Test Case Result: ', test_case_result_split_dataset)
print()

print('Running Test Case: Neural Network')
test_case_result_network = test_cases.test_network(VulnerabilityDetectorNetwork)
print('Test Case Result: ', test_case_result_network)
print()

if test_case_result_eval_function and test_case_result_split_dataset and test_case_result_network:
    main()
else:
    print('one or more test cases failed - quitting')
    quit()
