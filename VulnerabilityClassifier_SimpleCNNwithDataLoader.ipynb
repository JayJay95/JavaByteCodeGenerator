{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VulnerabilityClassifier_SimpleCNNwithDataLoader.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JayJay95/JavaByteCodeGenerator/blob/master/VulnerabilityClassifier_SimpleCNNwithDataLoader.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wbjUdmjC9gs5",
        "colab_type": "code",
        "outputId": "871c3ab9-4c5d-48dc-8fc6-ba3cd9ed6684",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uWxCvr1Z9kBw",
        "colab_type": "code",
        "outputId": "4e83cce6-d6d0-4acb-9f9c-402af49b983e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%cd /content/drive/My\\ Drive/After4thYear/Belfast/MSc\\ Cybersec/Research\\ Project/Colab Notebooks"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/After4thYear/Belfast/MSc Cybersec/Research Project/Colab Notebooks\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVM7weaq9nxm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import re\n",
        "import fnmatch\n",
        "import argparse\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data\n",
        "from torch.autograd import Variable\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import test_cases\n",
        "from copy import deepcopy\n",
        "import pdb\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eh_f2Dk49qMy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "clean_opseq_folder = '/content/drive/My Drive/After4thYear/Belfast/MSc Cybersec/Research Project/Colab Notebooks/Opseq/Clean_Opseq'\n",
        "vuln_opseq_folder = '/content/drive/My Drive/After4thYear/Belfast/MSc Cybersec/Research Project/Colab Notebooks/Opseq/Vuln_Opseq'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9mNGPASm9qq2",
        "colab_type": "code",
        "outputId": "5a4cb28a-16e4-4daf-c301-0799b866b11b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "parser = argparse.ArgumentParser(description='Vulnerability Classifier')\n",
        "# parser.add_argument('--max_opcode_seq_len', action='store', type=int, help='use different versions of network', default=8192)\n",
        "# parser.add_argument('--min_opcode_seq_len', action='store', type=int, help='use different versions of network', default=32)\n",
        "parser.add_argument('--lr', action='store', type=float, help='use different versions of network', default=1e-3)\n",
        "parser.add_argument('--epochs', action='store', type=int, help='use different versions of network', default=10)\n",
        "opt = parser.parse_args('')\n",
        "print(opt)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Namespace(epochs=10, lr=0.001)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3mcKD8wi9svS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RsLneDR69xty",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def find_files(files, dirs=[], extensions=[]): # recursively find files in directories\n",
        "    new_dirs = []\n",
        "    for d in dirs:\n",
        "        try:\n",
        "            new_dirs += [ os.path.join(d, f) for f in os.listdir(d)] # check in all directories except testcasesupport \n",
        "        except OSError:\n",
        "            if os.path.splitext(d)[1] in extensions:\n",
        "                files.append(d)\n",
        "\n",
        "    if new_dirs:\n",
        "        find_files(files, new_dirs, extensions)\n",
        "    else:\n",
        "        return  \n",
        "\n",
        "def read_file(filename):\n",
        "    opcode_count = 0\n",
        "    line_list = []\n",
        "    with open(filename, mode='rt', encoding='utf8') as f:\n",
        "        content = f.readlines()        \n",
        "    for line in content:\n",
        "        opcode_seq = []     \n",
        "        for c in range(0, len(line) - 1, 2):\n",
        "            #print(line[c:(c+2)],int(line[c:(c+2)], 16))\n",
        "            opcode_seq.append(int(line[c:(c+2)], 16) + 1) # add one here so that the zero'th embedding is reserved for 'blank' i.e. no instruction whatsoever not even no-op\t\t\t\n",
        "            opcode_count += 1\n",
        "            # to save training time we only read \n",
        "            # the first opt.max_opcode_seq_len opcodes of each file\n",
        "            \n",
        "            # if opcode_count >= opt.max_opcode_seq_len:\n",
        "            #     return opcode_seq\n",
        "        line_list.append(opcode_seq)\n",
        "    return line_list\n",
        "\n",
        "def read_dataset():\n",
        "    vuln = []\n",
        "    clean = []\n",
        "    clean_opseq_files = []\n",
        "    vuln_opseq_files = []\n",
        "    # min_file_len = opt.min_opcode_seq_len #ignore opcode seq files shorter than this\n",
        "    find_files(clean_opseq_files, dirs=[clean_opseq_folder], extensions=['.clean'])\n",
        "    for clean_file_pathname in clean_opseq_files:\n",
        "        tmp = read_file(clean_file_pathname)\n",
        "        # if len(tmp) >= min_file_len:\n",
        "        clean.append(tmp)\n",
        "    \n",
        "    find_files(vuln_opseq_files, dirs=[vuln_opseq_folder], extensions=['.vuln'])\n",
        "    for vuln_file_pathname in vuln_opseq_files:\n",
        "        tmp = read_file(vuln_file_pathname)\n",
        "        # if len(tmp) >= min_file_len:\n",
        "        vuln.append(tmp)\n",
        "    \n",
        "    # flatten vuln and clean lists\n",
        "    new_vuln = []\n",
        "    for x in vuln:\n",
        "        for y in x:\n",
        "            new_vuln.append(y)\n",
        "    \n",
        "    new_clean = []\n",
        "    for x in clean:\n",
        "        for y in x:\n",
        "            new_clean.append(y)\n",
        "    return new_vuln, new_clean"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTQPHjW19ycq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_dataset(vuln, clean):\n",
        "    #split the dataset into train, val, test sets\n",
        "    #return the concatenated dataset and\n",
        "    #indicies pointing to the train,val,test samples\n",
        "\n",
        "    vuln_label = 0\n",
        "    clean_label = 1\n",
        "    dataset = deepcopy(clean) + deepcopy(vuln) # concatenate original clean and original vuln samples\n",
        "    trial_dataset = clean + vuln\n",
        "    for list in trial_dataset:\n",
        "        if len(list) < 1000:\n",
        "            list.extend([0] * (1000- len(list)))\n",
        "    \n",
        "    final_vuln_list = []\n",
        "    for vuln_list in vuln:\n",
        "        inner_vuln_list = []\n",
        "        inner_vuln_list.append(vuln_list)\n",
        "        inner_vuln_list.append(vuln_label)\n",
        "        final_vuln_list.append(inner_vuln_list)\n",
        "             \n",
        "    final_clean_list = []\n",
        "    for clean_list in clean:\n",
        "        inner_clean_list = []\n",
        "        inner_clean_list.append(clean_list)\n",
        "        inner_clean_list.append(clean_label)\n",
        "        final_clean_list.append(inner_clean_list)\n",
        "    \n",
        "    # split vuln samples randomly\n",
        "    first_vuln_split = int(0.8 * len(final_vuln_list))\n",
        "    second_vuln_split = int(0.1 * len(final_vuln_list))\n",
        "    third_vuln_split = int(len(final_vuln_list) - (first_vuln_split + second_vuln_split))\n",
        "    vuln_training_dataset, vuln_validation_dataset, vuln_testing_dataset = torch.utils.data.random_split(final_vuln_list, [first_vuln_split, second_vuln_split, third_vuln_split])\n",
        "\n",
        "    # split clean samples randomly\n",
        "    first_clean_split = int(0.8 * len(final_clean_list))\n",
        "    second_clean_split = int(0.1*len(final_clean_list))\n",
        "    third_clean_split = int(len(final_clean_list) - (first_clean_split + second_clean_split))\n",
        "    clean_training_dataset, clean_validation_dataset, clean_testing_dataset = torch.utils.data.random_split(final_clean_list, [first_clean_split, second_clean_split, third_clean_split])\n",
        "\n",
        "    # merge both vuln & clean training sets\n",
        "    training_set = clean_training_dataset + vuln_training_dataset\n",
        "\n",
        "    # get indices and labels from training set\n",
        "    train_inds=[]\n",
        "    train_labels=[]\n",
        "    training_counter = 0\n",
        "    \n",
        "    for list in training_set:\n",
        "        train_inds.append(training_counter)\n",
        "        training_counter+=1\n",
        "        train_labels.append(list[1])\n",
        "        \n",
        "    # merge both vuln & clean validation sets\n",
        "    validation_set = clean_validation_dataset + vuln_validation_dataset\n",
        "\n",
        "    # get indices and labels from validation set\n",
        "    val_inds = []\n",
        "    val_labels = []\n",
        "    training_set_length = len(training_set)\n",
        "    for list in validation_set:\n",
        "        val_inds.append(training_set_length)\n",
        "        training_set_length += 1\n",
        "        val_labels.append(list[1])\n",
        "\n",
        "    # merge both vuln & clean testing sets\n",
        "    testing_set = clean_testing_dataset + vuln_testing_dataset\n",
        "\n",
        "    # get indices and labels from testing set\n",
        "    test_inds = []\n",
        "    test_labels = []\n",
        "    validation_set_length = len(training_set) + len(validation_set)\n",
        "    for list in testing_set:\n",
        "        test_inds.append(validation_set_length)\n",
        "        validation_set_length += 1\n",
        "        test_labels.append(list[1])\n",
        "    # pdb.set_trace()\n",
        "    return trial_dataset, training_set, validation_set, testing_set\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3gCm2uNC9_My",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class VulnerabilityDetectorNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(VulnerabilityDetectorNetwork, self).__init__()\n",
        "\n",
        "        self.num_of_embeddings = 205\n",
        "        self.embedding_dimension = 8\n",
        "        self.channels_in = 1\n",
        "        self.channels_out = 64\n",
        "        self.hidden_nodes = 16\n",
        "        self.kernel_height_dimension = 8\n",
        "        self.kernel_width_dimension = 8\n",
        "        self.padding_height_dimension = 0\n",
        "        self.padding_width_dimension = 0\n",
        "        self.features_out = 2\n",
        "\n",
        "        self.emb1 = nn.Embedding(self.num_of_embeddings, self.embedding_dimension)\n",
        "        self.conv1 = nn.Conv2d(self.channels_in, self.channels_out, kernel_size=(self.kernel_height_dimension, self.kernel_width_dimension),\n",
        "            padding=(self.padding_height_dimension, self.padding_width_dimension))\n",
        "        self.lin1 = nn.Linear(self.channels_out, self.hidden_nodes)\n",
        "        self.lin2 = nn.Linear(self.hidden_nodes, self.features_out)\n",
        "        self.drop_out = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.emb1(x)\n",
        "#         print(x.shape)\n",
        "        x = x.unsqueeze(0)\n",
        "#         print(x.shape)\n",
        "        x = self.conv1(x)\n",
        "#         print(x.shape)\n",
        "        x = F.relu(x)\n",
        "#         print(x.shape)\n",
        "        x = self.drop_out(x)\n",
        "#         print(x.shape)\n",
        "        x = torch.max(x,2)[0]\n",
        "#         print(x.shape)\n",
        "        x = x.squeeze(2)\n",
        "#         print(x.shape)\n",
        "        x = self.lin1(x)\n",
        "#         print(x.shape)\n",
        "        x = self.lin2(x)\n",
        "#         print(x.shape)\n",
        "        \n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yRKTIctC-Czz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate_network_performance(predictions, ground_truth):\n",
        "    # given an array with the predicted values and the correct values\n",
        "    # calculate the precision, recall and f-score\n",
        "    cm = get_confusion_matrix(predictions, ground_truth)\n",
        "    TP = cm[0][0]\n",
        "    FP = cm[0][1]\n",
        "    FN = cm[1][0]\n",
        "    TN = cm[1][1]\n",
        "\n",
        "    precision = TP/(TP+FP)\n",
        "    recall = TP/(TP+FN)\n",
        "    classification_accuracy = (TP+TN) / (TP + TN + FP + FN)\n",
        "    f_score = 2 * ((precision*recall)/(precision + recall))\n",
        "\n",
        "    return precision, recall, f_score, classification_accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ko5Q7lMZ-FAh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_confusion_matrix(preds, truth):\n",
        "    K = len(np.unique(truth)) # Number of classes \n",
        "    result = np.zeros((K, K))\n",
        "    for i in range(len(truth)):\n",
        "        result[preds[i]][truth[i]] += 1\n",
        "    confusion_matrix = result\n",
        "    return confusion_matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jUveFs2W-HHW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_network(net, tvtLoader):\n",
        "    \n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    test_loss = 0\n",
        "    predictions = []\n",
        "    labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "#         for i in range(len(inds)):\n",
        "        for i, (imgs, lbls) in enumerate(tvtLoader):\n",
        "            net.eval()\n",
        "            imgs = torch.stack(imgs)\n",
        "            i_list = imgs.tolist()\n",
        "            flat_list = [item for sublist in i_list for item in sublist]\n",
        "            images_tensor = torch.LongTensor(flat_list)\n",
        "            \n",
        "            output = net(images_tensor.unsqueeze(0))\n",
        "            val,idx = torch.max(output,1)\n",
        "            predictions.append(idx.item())\n",
        "            labels.append(lbls)\n",
        "\n",
        "            test_loss = criterion(output, lbls).item()\n",
        "\n",
        "        precision, recall, f_score, classification_accuracy = evaluate_network_performance(predictions, labels)\n",
        "\n",
        "    return precision, recall, f_score, classification_accuracy\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y5R2cVAu-WDu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_network(net, trainLoader, valLoader):\n",
        "    \n",
        "    optimizer = optim.SGD(net.parameters(), lr=opt.lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    num_training_epochs = opt.epochs\n",
        "\n",
        "\n",
        "    for e in range(num_training_epochs):\n",
        "        randperm = torch.randperm(len(trainLoader))\n",
        "        running_loss = 0\n",
        "\n",
        "        for i, (images, labels) in enumerate(trainLoader):\n",
        "            net.train()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            images = torch.stack(images)\n",
        "            i_list = images.tolist()\n",
        "            flat_list = [item for sublist in i_list for item in sublist]\n",
        "            images_tensor = torch.LongTensor(flat_list)\n",
        "            \n",
        "            net_output = net(images_tensor.unsqueeze(0))\n",
        "            loss = criterion(net_output, labels)\n",
        "            running_loss += loss.item()\n",
        "            \n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        epoch_loss = running_loss/len(trainLoader)\n",
        "\n",
        "        print('iteration ', e, 'loss ', epoch_loss)\n",
        "        print('          prec rec  f1   acc')\n",
        "\n",
        "        precision, recall, f_score, classification_accuracy = test_network(net, valLoader)\n",
        "        print('val set  ',\"{0:.2f}\".format(precision),\"{0:.2f}\".format(recall),\"{0:.2f}\".format(f_score),\"{0:.2f}\".format(classification_accuracy))\n",
        "\n",
        "        precision, recall, f_score, classification_accuracy = test_network(net, trainLoader)\n",
        "        print('train set',\"{0:.2f}\".format(precision),\"{0:.2f}\".format(recall),\"{0:.2f}\".format(f_score),\"{0:.2f}\".format(classification_accuracy))\n",
        "        print()\n",
        "\n",
        "    return net\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ynVneEWU-Y9u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def createLoaders(train_set, val_set, test_set):\n",
        "    trainLoader = torch.utils.data.DataLoader(train_set, batch_size = 1, shuffle = True)\n",
        "    valLoader = torch.utils.data.DataLoader(val_set, batch_size = 1, shuffle = True)\n",
        "    testLoader = torch.utils.data.DataLoader(test_set, batch_size = 1, shuffle = False)    \n",
        "    \n",
        "    return trainLoader, valLoader, testLoader\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KoRlwT8G-qtZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def main():\n",
        "    # create_clean_opseq_files() # should only run the first time this code is ran\n",
        "    # print('Created clean opseq')\n",
        "    # create_vuln_opseq_files() # should only run the first time this code is ran\n",
        "    # print('Created vuln opseq')\n",
        "    vuln, clean = read_dataset()\n",
        "\n",
        "    dataset, train_set, val_set, test_set = split_dataset(vuln, clean)\n",
        "    trainLoader, valLoader, testLoader = createLoaders(train_set, val_set, test_set)\n",
        "    net = VulnerabilityDetectorNetwork()\n",
        "    # net.to(device)\n",
        "    print(net)\n",
        "    print('started training network')\n",
        "    before = time.time()\n",
        "    net = train_network(net, trainLoader, valLoader)\n",
        "    \n",
        "    print('started testing network')\n",
        "    precision, recall, f_score, classification_accuracy = test_network(net, testLoader)\n",
        "    print('test set',\"{0:.2f}\".format(precision),\"{0:.2f}\".format(recall),\"{0:.2f}\".format(f_score),\"{0:.2f}\".format(classification_accuracy))\n",
        "    print()\n",
        "    eval_time = time.time() - before\n",
        "    print (\"Time to run: \", eval_time)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uWh492Xi-uYS",
        "colab_type": "code",
        "outputId": "d53db9dd-91bd-4cc4-f725-c75403a7883a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# if test_case_result_eval_function and test_case_result_split_dataset and test_case_result_network:\n",
        "# if test_case_result_eval_function and test_case_result_split_dataset:\n",
        "main()\n",
        "# else:\n",
        "#     print('one or more test cases failed - quitting')\n",
        "#     quit()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "VulnerabilityDetectorNetwork(\n",
            "  (emb1): Embedding(205, 8)\n",
            "  (conv1): Conv2d(1, 64, kernel_size=(8, 8), stride=(1, 1))\n",
            "  (lin1): Linear(in_features=64, out_features=16, bias=True)\n",
            "  (lin2): Linear(in_features=16, out_features=2, bias=True)\n",
            "  (drop_out): Dropout(p=0.5)\n",
            ")\n",
            "started training network\n",
            "iteration  0 loss  0.23158756157847843\n",
            "          prec rec  f1   acc\n",
            "val set   0.91 0.97 0.94 0.94\n",
            "train set 0.91 0.97 0.94 0.94\n",
            "\n",
            "iteration  1 loss  0.1680671176919693\n",
            "          prec rec  f1   acc\n",
            "val set   0.92 0.96 0.94 0.94\n",
            "train set 0.92 0.96 0.94 0.94\n",
            "\n",
            "iteration  2 loss  0.15083621893371532\n",
            "          prec rec  f1   acc\n",
            "val set   0.93 0.96 0.94 0.94\n",
            "train set 0.93 0.96 0.94 0.94\n",
            "\n",
            "iteration  3 loss  0.14021341492118472\n",
            "          prec rec  f1   acc\n",
            "val set   0.93 0.96 0.95 0.95\n",
            "train set 0.94 0.96 0.95 0.95\n",
            "\n",
            "iteration  4 loss  0.13294342565732845\n",
            "          prec rec  f1   acc\n",
            "val set   0.94 0.95 0.95 0.95\n",
            "train set 0.94 0.95 0.95 0.95\n",
            "\n",
            "iteration  5 loss  0.12866214948588706\n",
            "          prec rec  f1   acc\n",
            "val set   0.92 0.98 0.95 0.95\n",
            "train set 0.92 0.98 0.95 0.95\n",
            "\n",
            "iteration  6 loss  0.12466277225249717\n",
            "          prec rec  f1   acc\n",
            "val set   0.94 0.96 0.95 0.95\n",
            "train set 0.94 0.95 0.95 0.95\n",
            "\n",
            "iteration  7 loss  0.12089595940138997\n",
            "          prec rec  f1   acc\n",
            "val set   0.92 0.98 0.95 0.95\n",
            "train set 0.92 0.98 0.95 0.95\n",
            "\n",
            "iteration  8 loss  0.11804292081864781\n",
            "          prec rec  f1   acc\n",
            "val set   0.92 0.98 0.95 0.95\n",
            "train set 0.92 0.98 0.95 0.95\n",
            "\n",
            "iteration  9 loss  0.11721375336246546\n",
            "          prec rec  f1   acc\n",
            "val set   0.94 0.95 0.95 0.94\n",
            "train set 0.94 0.95 0.95 0.95\n",
            "\n",
            "started testing network\n",
            "test set 0.94 0.94 0.94 0.94\n",
            "\n",
            "Time to run:  24693.50425052643\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}